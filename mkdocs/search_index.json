{
    "docs": [
        {
            "location": "/", 
            "text": "Genetics 711/811 | Bioinformatics and Genomics\n\n\n\n\n\n\nContact Information for Prof. Matthew MacManes\n\n\n\n\nOffice: 189 Rudman Hall\n\n\nEmail (preferred): \n\n\nTwitter: @MacManes\n\n\nOffice hours: Tues/Fri 8-9AM in 189 Rudman, or by appt. Schedule at \nhttp://genomebio.org/office-hours/\n\n\n\n\n\n\n\n\nSlack\n\n\n\n\n\n\nLecture: MWF 11:10-12:00PM Rudman G89\n\n\n\n\n\n\nLab: Friday 1:10PM - 3PM or 3:10-5PM in Hewitt 301.\n\n\n\n\n\n\nSyllabus\n\n\n\n\n\n\nAWS Resources\n\n\n\n\n\n\nLecture Materials.\n\n\n\n\nWelcome Lecture\n\n\nLecture 2\n\n\nLecture 3\n\n\nLecture 4\n\n\nLecture 6\n\n\nLecture 7\n\n\nLecture 9\n\n\nLecture 10\n\n\nLecture 11\n\n\nLecture 12\n\n\nLecture 13\n\n\nLecture 14\n\n\n\n\n\n\n\n\nLab lessons\n\n\n\n\nCommand Line Bootcamp\n\n\nLaunch an EC2 Instance\n\n\nGetting Started\n\n\nWEEK 2 LAB\n\n\nWeek 3 lab\n\n\nWeek 4 lab\n\n\nWeek 5 lab\n\n\n\n\n\n\n\n\nReading\n\n\n\n\nFolder o' Papers\n You will need to sign in with UNH ID.\n\n\nWeek 1: Goodwin2016 and Ashley\n\n\nWeek 2: Altschul and Kelley\n\n\nWeek 3: \"Week2\" and Katoh\n\n\nWhat is a HMM\n\n\nHMMs in Biology\n\n\n\n\n\n\n\n\n\n\n\n\nHomework\n\n\n\n\n\n\nExams\n\n\n\n\nExam 1 Practice\n\n\nUtter Panic: :doc:\nlabs/mock.exam2\n\n\nPractice for exam2: :doc:\nlabs/Exam2_Practice\n\n\nThe real deal exam 2: :doc:\nlabs/exam2", 
            "title": "Home"
        }, 
        {
            "location": "/#genetics-711811-bioinformatics-and-genomics", 
            "text": "Contact Information for Prof. Matthew MacManes   Office: 189 Rudman Hall  Email (preferred):   Twitter: @MacManes  Office hours: Tues/Fri 8-9AM in 189 Rudman, or by appt. Schedule at  http://genomebio.org/office-hours/     Slack    Lecture: MWF 11:10-12:00PM Rudman G89    Lab: Friday 1:10PM - 3PM or 3:10-5PM in Hewitt 301.    Syllabus    AWS Resources    Lecture Materials.   Welcome Lecture  Lecture 2  Lecture 3  Lecture 4  Lecture 6  Lecture 7  Lecture 9  Lecture 10  Lecture 11  Lecture 12  Lecture 13  Lecture 14     Lab lessons   Command Line Bootcamp  Launch an EC2 Instance  Getting Started  WEEK 2 LAB  Week 3 lab  Week 4 lab  Week 5 lab     Reading   Folder o' Papers  You will need to sign in with UNH ID.  Week 1: Goodwin2016 and Ashley  Week 2: Altschul and Kelley  Week 3: \"Week2\" and Katoh  What is a HMM  HMMs in Biology       Homework    Exams   Exam 1 Practice  Utter Panic: :doc: labs/mock.exam2  Practice for exam2: :doc: labs/Exam2_Practice  The real deal exam 2: :doc: labs/exam2", 
            "title": "Genetics 711/811 | Bioinformatics and Genomics"
        }, 
        {
            "location": "/AWS/", 
            "text": "How To Get Going with AWS\n\n\n\n\n\n\nApply for an educational account at \nhttps://aws.amazon.com/education/awseducate/apply/\n. This will give you $100 credit and other benefits.\n\n\n\n\n\n\nIMPORTANT\n: When registering, you do not want the \"AWS Educate Starter Account\". You will need to type in your credit card, but no charged will be incurred (assuming you follow the directions).\n\n\n\n\nYou want the personal account, and no support package (They call this the BASIC plan). \n\n\nPart of the registration process will require you answer an automated phone call from Amazon that serves to verify your human being-ness...\n\n\n\n\nYou need to do this even if you have a regular Amazon account.\n\n\n\n\n\n\nLaunch an EC2 Instance", 
            "title": "AWS"
        }, 
        {
            "location": "/AWS/#how-to-get-going-with-aws", 
            "text": "Apply for an educational account at  https://aws.amazon.com/education/awseducate/apply/ . This will give you $100 credit and other benefits.    IMPORTANT : When registering, you do not want the \"AWS Educate Starter Account\". You will need to type in your credit card, but no charged will be incurred (assuming you follow the directions).   You want the personal account, and no support package (They call this the BASIC plan).   Part of the registration process will require you answer an automated phone call from Amazon that serves to verify your human being-ness...   You need to do this even if you have a regular Amazon account.    Launch an EC2 Instance", 
            "title": "How To Get Going with AWS"
        }, 
        {
            "location": "/Syllabus/", 
            "text": "Genetics 711/811 | Bioinformatics and Genomics | Syllabus\n\n\n0. Course Description:\n\n\nThe methods, applications, and implications of genomics - the analysis of whole genomes. Microbial, plant and animal genomics are addressed, as well as medical, ethical and legal implications. The lab provides exposure and experience on a range of bioinformatics approaches - the computer applications used in genome analysis.\n\n\n1. Contact Information for Professor Matthew MacManes\n\n\n\n\nOffice: 189 Rudman Hall\n\n\nPhone: 603-862-4052\n\n\nEmail (preferred): Matthew.MacManes@unh.edu\n\n\nTwitter: @MacManes\n\n\nOffice hours: Tues/Fri 8-9am in 189 Rudman, or by appt. Just show up, or schedule at \nhttp://genomebio.org/office-hours/\n\n\n\n\n2. Contact Information for Dr Raymond Holsapple\n\n\n\n\nOffice: 193 Rudman Hall\n\n\nEmail: rh1054@wildcats.edu\n\n\nOffice hours: Thursday 10-11AM in 193 Rudman\n\n\n\n\n3. Lecture: MWF 11:10-12:00PM Rudman G89\n\n\n4. Lab: Friday Either 1:10-3:00PM or 3:10-5:00PM in Hewitt 301\n\n\n5. Web Resources:\n\n\n\n\nSlack\n\n\nCanvas: I will use this site for grades, only!\n\n\nGoogle: everything you'll learn this semester has been discussed on the internet. Use it!\n\n\nhttp://SeqAnswers.com\n: The forums are really great.\n\n\nBioStars: \nhttps://www.biostars.org/t/\n\n\nSummer workshop website that has lots of good info: \nhttp://angus.readthedocs.org/en/2016\n\n\n\n\n6. Assignments and Exams\n\n\nIrregular Assignments:\n are assigned in class and/or lab, and are meant to enhance your understanding of the material. They are typically ungraded, but doing them will help to improve your grade. Examples may include extra reading, computer work, or other things.\n\n\nReading\n is assigned weekly in the form of primary literature. Each student will present a research paper to the class. This paper presentation is meant to be relatively informal, but will increase the depth of your understanding. Each presentation will be ~10 minutes, with a few minutes for questions. Schedule for this is TBD, but expect these to start before the 1st exam. \nFolder o' Papers\n\n\nExams:\n There will be 2 exams covering both lecture and lab material. Each will be worth 100 points. The dates for these will be October 7 and November 18. Makeup exams will be permitted only under extreme documented circumstances, or by prior approval (\n 1 week) from Prof. MacManes.\n\n\nFinal Project:\n The final project will consist of an oral presentation and written report (e.g., the methods section) related to an assembly project. Projects must incorporate an implementation of the computational techniques we've learned about. The final project will be worth 100 points (75 written/25 oral). Oral presentations will occur during the last 3 days of class. Written reports will be due on the last day of class. More details will be provided later in the semester.\n\n\nParticipation:\n Attendance to the lab portion of the class is critical. We have 13 lab sessions. 10 points each, maximum of 120 points may be earned. One lab may be missed without penalty. Additional missed labs may be made up with prior approval (\n 1 week) from Prof. MacManes\n\n\n7. Materials Needed\n\n\nAmazon Web services:\n For lab exercises, we will use the Amazon Computer Cloud (EC2). You will need to set up an account during the first lab, and will be given $100 to pay for the analyses you will run this semester. The EC2 interface will work properly in OSX and Linux operating systems. For those of you with Windows computers, you will need to use the OSX computers in the computer lab. Work outside the class can be accomplished after installation of a terminal emulator (MobaXterm is best).\n\n\n8. Grades\n\n\nThe grade scale is: 93-100=A; 90-92.99=A-; 87-89.99=B+; 83-86.99=B; 80-82.99=B-; 77-79.99=C+; 73-76.99=C; 70-72.99=C-; 67-69.99=D+; 63-66.99=D; 60-62.99=D-; Below 60=F\n\n\n\n\n\n\n\n\nItem\n\n\nMaximum Points\n\n\n\n\n\n\n\n\n\n\nExams\n\n\n200\n\n\n\n\n\n\nFinal Project\n\n\n100\n\n\n\n\n\n\nPaper Presentation\n\n\n25\n\n\n\n\n\n\nParticipation\n\n\n120\n\n\n\n\n\n\nTotal Points\n\n\n445\n\n\n\n\n\n\n\n\n9. Course Policies\n\n\nStudent conduct:\n Honesty is a core value at the University of New Hampshire.  The members of this academic community require and expect one another to conduct themselves with integrity.  The Student Rights, Rules and Responsibilities handbook (www.unh.edu/student/rights) explains UNH's expectation for academic honesty and defines those actions that constitute academic misconduct with regard to exams, homework, plagiarism, computers, etc.  The penalty for the first incident of cheating, plagiarism or other breaches of the university's academic honesty policies will be an automatic F grade for that assignment.  A second infraction will result in an F grade for the class.  The Dean's office will be notified and dismissal from the university could result.\n\n\nDisability Services for Students:\n The University of New Hampshire is committed to providing students with documented disabilities equal access to all university programs and facilities.  If you have a disability requiring accommodation, you must register with Disability Services for Students (DSS).  Contact DSS at 862-2607.  If you have received an Accommodation Letter for this course from DSS, please meet with Prof. MacManes privately to review those accommodations.\n\n\n10. How to get an A.\n\n\nReceiving an A in this shall should be really easy (I mean it!), assuming you follow these basic guidelines.\n\n\n\n\nCome to class and lab, pay attention, be interactive: Active learning far outcompetes passive, so while coming to class itself is good, interacting/asking questions will be much better.\n\n\nAsk questions when your confused. Come to office hours, or schedule a time to meet.\n\n\nDon't cram! Study a little, several times per week. You will remember more, and the exams will be less stressful.\n\n\nRead the papers and use (with caution) online resources.\n\n\n\n\n11. Course Schedule - Subject to Change\n\n\n\n\n\n\n\n\nWeek\n\n\nReading\n\n\nTopic\n\n\nLab\n\n\n\n\n\n\n\n\n\n\n29Aug\n\n\n\n\nIntro to Bioinformatics and Molecular Evolution\n\n\nUNIX\n\n\n\n\n\n\n5Sept (No Class Mon)\n\n\n\n\nFinding Data \n Pairwise Alignment\n\n\nBLAST\n\n\n\n\n\n\n12Sept\n\n\n\n\nBLAST\n\n\nAlignment\n\n\n\n\n\n\n19Sept (No Class Wed)\n\n\n\n\nAdvanced Search (BLAT/HMM/others)\n\n\nHMMER/RepeatMasker\n\n\n\n\n\n\n26Sept\n\n\n\n\nMultiple Sequence Alignment\n\n\nfastA/fastQ processing\n\n\n\n\n\n\n03Oct\n\n\n\n\nSequence Read Analysis\n\n\nTranscriptome Analyses1\n\n\n\n\n\n\nEXAM OCT 07\n\n\n\n\n\n\n\n\n\n\n\n\n10Oct\n\n\n\n\nRNA and Transcriptomics\n\n\nTranscriptome Analyses2\n\n\n\n\n\n\n17Oct\n\n\n\n\nGene Expression\n\n\nMeasuring Gene Expression\n\n\n\n\n\n\n24Oct\n\n\n\n\nGenome Evolution\n\n\nGenome Assembly\n\n\n\n\n\n\n31Oct\n\n\n\n\nGenome Assembly\n\n\nGenome Analyses\n\n\n\n\n\n\n07Nov (No Class Fri)\n\n\n\n\nGenome Analyses\n\n\nLong Reads\n\n\n\n\n\n\n14Nov\n\n\n\n\nPopulation Genomics\n\n\nBacterial Genome Annot.\n\n\n\n\n\n\nEXAM NOV 18\n\n\n\n\n\n\n\n\n\n\n\n\n21Nov (no Class Wed/Fri)\n\n\n\n\nPersonal and Medical Genomics\n\n\nProjects\n\n\n\n\n\n\n28Nov\n\n\nTBD\n\n\nProjects\n\n\n\n\n\n\n\n\n05Dec\n\n\n\n\nEthics \n Final Project Presentations\n\n\nNo Lab", 
            "title": "Syllabus"
        }, 
        {
            "location": "/Syllabus/#genetics-711811-bioinformatics-and-genomics-syllabus", 
            "text": "", 
            "title": "Genetics 711/811 | Bioinformatics and Genomics | Syllabus"
        }, 
        {
            "location": "/Syllabus/#0-course-description", 
            "text": "The methods, applications, and implications of genomics - the analysis of whole genomes. Microbial, plant and animal genomics are addressed, as well as medical, ethical and legal implications. The lab provides exposure and experience on a range of bioinformatics approaches - the computer applications used in genome analysis.", 
            "title": "0. Course Description:"
        }, 
        {
            "location": "/Syllabus/#1-contact-information-for-professor-matthew-macmanes", 
            "text": "Office: 189 Rudman Hall  Phone: 603-862-4052  Email (preferred): Matthew.MacManes@unh.edu  Twitter: @MacManes  Office hours: Tues/Fri 8-9am in 189 Rudman, or by appt. Just show up, or schedule at  http://genomebio.org/office-hours/", 
            "title": "1. Contact Information for Professor Matthew MacManes"
        }, 
        {
            "location": "/Syllabus/#2-contact-information-for-dr-raymond-holsapple", 
            "text": "Office: 193 Rudman Hall  Email: rh1054@wildcats.edu  Office hours: Thursday 10-11AM in 193 Rudman", 
            "title": "2. Contact Information for Dr Raymond Holsapple"
        }, 
        {
            "location": "/Syllabus/#3-lecture-mwf-1110-1200pm-rudman-g89", 
            "text": "", 
            "title": "3. Lecture: MWF 11:10-12:00PM Rudman G89"
        }, 
        {
            "location": "/Syllabus/#4-lab-friday-either-110-300pm-or-310-500pm-in-hewitt-301", 
            "text": "", 
            "title": "4. Lab: Friday Either 1:10-3:00PM or 3:10-5:00PM in Hewitt 301"
        }, 
        {
            "location": "/Syllabus/#5-web-resources", 
            "text": "Slack  Canvas: I will use this site for grades, only!  Google: everything you'll learn this semester has been discussed on the internet. Use it!  http://SeqAnswers.com : The forums are really great.  BioStars:  https://www.biostars.org/t/  Summer workshop website that has lots of good info:  http://angus.readthedocs.org/en/2016", 
            "title": "5. Web Resources:"
        }, 
        {
            "location": "/Syllabus/#6-assignments-and-exams", 
            "text": "Irregular Assignments:  are assigned in class and/or lab, and are meant to enhance your understanding of the material. They are typically ungraded, but doing them will help to improve your grade. Examples may include extra reading, computer work, or other things.  Reading  is assigned weekly in the form of primary literature. Each student will present a research paper to the class. This paper presentation is meant to be relatively informal, but will increase the depth of your understanding. Each presentation will be ~10 minutes, with a few minutes for questions. Schedule for this is TBD, but expect these to start before the 1st exam.  Folder o' Papers  Exams:  There will be 2 exams covering both lecture and lab material. Each will be worth 100 points. The dates for these will be October 7 and November 18. Makeup exams will be permitted only under extreme documented circumstances, or by prior approval (  1 week) from Prof. MacManes.  Final Project:  The final project will consist of an oral presentation and written report (e.g., the methods section) related to an assembly project. Projects must incorporate an implementation of the computational techniques we've learned about. The final project will be worth 100 points (75 written/25 oral). Oral presentations will occur during the last 3 days of class. Written reports will be due on the last day of class. More details will be provided later in the semester.  Participation:  Attendance to the lab portion of the class is critical. We have 13 lab sessions. 10 points each, maximum of 120 points may be earned. One lab may be missed without penalty. Additional missed labs may be made up with prior approval (  1 week) from Prof. MacManes", 
            "title": "6. Assignments and Exams"
        }, 
        {
            "location": "/Syllabus/#7-materials-needed", 
            "text": "Amazon Web services:  For lab exercises, we will use the Amazon Computer Cloud (EC2). You will need to set up an account during the first lab, and will be given $100 to pay for the analyses you will run this semester. The EC2 interface will work properly in OSX and Linux operating systems. For those of you with Windows computers, you will need to use the OSX computers in the computer lab. Work outside the class can be accomplished after installation of a terminal emulator (MobaXterm is best).", 
            "title": "7. Materials Needed"
        }, 
        {
            "location": "/Syllabus/#8-grades", 
            "text": "The grade scale is: 93-100=A; 90-92.99=A-; 87-89.99=B+; 83-86.99=B; 80-82.99=B-; 77-79.99=C+; 73-76.99=C; 70-72.99=C-; 67-69.99=D+; 63-66.99=D; 60-62.99=D-; Below 60=F     Item  Maximum Points      Exams  200    Final Project  100    Paper Presentation  25    Participation  120    Total Points  445", 
            "title": "8. Grades"
        }, 
        {
            "location": "/Syllabus/#9-course-policies", 
            "text": "Student conduct:  Honesty is a core value at the University of New Hampshire.  The members of this academic community require and expect one another to conduct themselves with integrity.  The Student Rights, Rules and Responsibilities handbook (www.unh.edu/student/rights) explains UNH's expectation for academic honesty and defines those actions that constitute academic misconduct with regard to exams, homework, plagiarism, computers, etc.  The penalty for the first incident of cheating, plagiarism or other breaches of the university's academic honesty policies will be an automatic F grade for that assignment.  A second infraction will result in an F grade for the class.  The Dean's office will be notified and dismissal from the university could result.  Disability Services for Students:  The University of New Hampshire is committed to providing students with documented disabilities equal access to all university programs and facilities.  If you have a disability requiring accommodation, you must register with Disability Services for Students (DSS).  Contact DSS at 862-2607.  If you have received an Accommodation Letter for this course from DSS, please meet with Prof. MacManes privately to review those accommodations.", 
            "title": "9. Course Policies"
        }, 
        {
            "location": "/Syllabus/#10-how-to-get-an-a", 
            "text": "Receiving an A in this shall should be really easy (I mean it!), assuming you follow these basic guidelines.   Come to class and lab, pay attention, be interactive: Active learning far outcompetes passive, so while coming to class itself is good, interacting/asking questions will be much better.  Ask questions when your confused. Come to office hours, or schedule a time to meet.  Don't cram! Study a little, several times per week. You will remember more, and the exams will be less stressful.  Read the papers and use (with caution) online resources.", 
            "title": "10. How to get an A."
        }, 
        {
            "location": "/Syllabus/#11-course-schedule-subject-to-change", 
            "text": "Week  Reading  Topic  Lab      29Aug   Intro to Bioinformatics and Molecular Evolution  UNIX    5Sept (No Class Mon)   Finding Data   Pairwise Alignment  BLAST    12Sept   BLAST  Alignment    19Sept (No Class Wed)   Advanced Search (BLAT/HMM/others)  HMMER/RepeatMasker    26Sept   Multiple Sequence Alignment  fastA/fastQ processing    03Oct   Sequence Read Analysis  Transcriptome Analyses1    EXAM OCT 07       10Oct   RNA and Transcriptomics  Transcriptome Analyses2    17Oct   Gene Expression  Measuring Gene Expression    24Oct   Genome Evolution  Genome Assembly    31Oct   Genome Assembly  Genome Analyses    07Nov (No Class Fri)   Genome Analyses  Long Reads    14Nov   Population Genomics  Bacterial Genome Annot.    EXAM NOV 18       21Nov (no Class Wed/Fri)   Personal and Medical Genomics  Projects    28Nov  TBD  Projects     05Dec   Ethics   Final Project Presentations  No Lab", 
            "title": "11. Course Schedule - Subject to Change"
        }, 
        {
            "location": "/lab_lessons/Lab10_bacterial_genome_assembly/", 
            "text": "Lab 10: Bacterial Genome Assembly\n\n\n\n\nDuring this lab, we will acquaint ourselves with\u00a0Genome Assembly using SPAdes. We will assembly the genome of \nE. coli\n. The data are taken from here:\u00a0\nhttps://github.com/lexnederbragt/INF-BIOx121_fall2014_de_novo_assembly/blob/master/Sources.md\n.\n\n\n1. Install software and download data\n\n\n2. Error correct, quality and adapter trim data sets.\n\n\n\n\nAssemble\n\n\n\n\n-\n\n\nThe SPAdes manuscript: \nhttp://www.ncbi.nlm.nih.gov/pubmed/22506599\n\nThe SPAdes manual: \nhttp://spades.bioinf.spbau.ru/release3.1.1/manual.html\n\nSPAdes website: \nhttp://bioinf.spbau.ru/spades\n\nABySS webpage:\u00a0\nhttps://github.com/bcgsc/abyss\n\n\n-\n\n\n\n\nStep 1: Launch and AMI. For this exercise,\u00a0we will use a\u00a0\nc3.2xlarge\n\u00a0(note different instance type). Remember to change the permission of your key code \nchmod 400 ~/Downloads/????.pem\n (change ????.pem to whatever you named it)\n\n\n\n\nssh -i ~/Downloads/?????.pem ubuntu@ec2-???-???-???-???.compute-1.amazonaws.com\n\n\n\n\n\n\n\nUpdate Software\n\n\n\n\nsudo bash\napt-get update\n\n\n\n\n\n\n\nInstall updates\n\n\n\n\napt-get -y upgrade\n\n\n\n\n\n\n\nInstall other software\n\n\n\n\napt-get -y install subversion tmux git curl libncurses5-dev gcc make g++ python-dev unzip dh-autoreconf zlib1g-dev libboost1.55-dev sparsehash openmpi*\n\n\n\n\n\n\n\nInstall SPAdes\n\n\n\n\ncd $HOME\nwget http://spades.bioinf.spbau.ru/release3.1.1/SPAdes-3.1.1-Linux.tar.gz\ntar -zxf SPAdes-3.1.1-Linux.tar.gz\ncd SPAdes-3.1.1-Linux\nPATH=$PATH:$(pwd)/bin\n\n\n\n\n\n\n\nInstall ABySS\n\n\n\n\ncd $HOME\ngit clone https://github.com/bcgsc/abyss.git\ncd abyss\n./autogen.sh\n./configure --enable-maxk=128 --prefix=/usr/local/ --with-mpi=/usr/lib/openmpi/\nmake -j4\nmake all install\n\n\n\n-\n\n\n\n\nInstall a script for assembly evaluation.\n\n\n\n\ngit clone https://github.com/lexnederbragt/sequencetools.git\ncd sequencetools/\nPATH=$PATH:$(pwd)\n\n\n\n\n\nDownload and unpack the data\n\n\n\n\ncd /mnt\nwget https://s3.amazonaws.com/gen711/ecoli_data.tar.gz\ntar -zxf ecoli_data.tar.gz\n\n\n\n\n\nAssembly. Try this with different data combos (with mate pair data, without, with minION data and without, etc). Remember to name your assemblies something different using the \n-o\n flag. Spades has a built-in error correction tool (remove \n--only-assembler\n). Does 'double error correction seem to make a difference?'.\n\n\n\n\nmkdir /mnt/spades\ncd /mnt/spades\n\nspades.py -t 8 -m 15 --only-assembler --mp1-rf -k 127 \\\n--pe1-1 /mnt/ecoli_pe.1.fq \\\n--pe1-2 /mnt/ecoli_pe.2.fq \\\n--mp1-1 /mnt/nextera.1.fq \\\n--mp1-2 /mnt/nextera.2.fq \\\n--pacbio /mnt/minion.data.fasta \\\n-o Ecoli_all_data\n\n\n\n\n\n\n\nEvaluate Assemblies\n\n\n\n\nabyss-fac Ecoli_all_data/scaffolds.fasta\n\n#take a closer look.\n\nassemblathon_stats.pl Ecoli_all_data/scaffolds.fasta\n\n\n\n\n\nAssembling with ABySS (optional)\n\n\n\n\nmkdir /mnt/abyss\ncd /mnt/abyss\n\nabyss-pe np=8 k=127 name=ecoli lib='pe1' mp='mp1' long='minion' \\\npe1='/mnt/ecoli_pe.1.fq /mnt/ecoli_pe.2.fq' \\\nmp1='/mnt/nextera.1.fq /mnt/nextera.2.fq' \\\nminion='/mnt/minion.data.fasta' mp1_l=30", 
            "title": "Lab10 bacterial genome assembly"
        }, 
        {
            "location": "/lab_lessons/Lab10_bacterial_genome_assembly/#lab-10-bacterial-genome-assembly", 
            "text": "During this lab, we will acquaint ourselves with\u00a0Genome Assembly using SPAdes. We will assembly the genome of  E. coli . The data are taken from here:\u00a0 https://github.com/lexnederbragt/INF-BIOx121_fall2014_de_novo_assembly/blob/master/Sources.md .  1. Install software and download data  2. Error correct, quality and adapter trim data sets.   Assemble   -  The SPAdes manuscript:  http://www.ncbi.nlm.nih.gov/pubmed/22506599 \nThe SPAdes manual:  http://spades.bioinf.spbau.ru/release3.1.1/manual.html \nSPAdes website:  http://bioinf.spbau.ru/spades \nABySS webpage:\u00a0 https://github.com/bcgsc/abyss  -   Step 1: Launch and AMI. For this exercise,\u00a0we will use a\u00a0 c3.2xlarge \u00a0(note different instance type). Remember to change the permission of your key code  chmod 400 ~/Downloads/????.pem  (change ????.pem to whatever you named it)   ssh -i ~/Downloads/?????.pem ubuntu@ec2-???-???-???-???.compute-1.amazonaws.com    Update Software   sudo bash\napt-get update    Install updates   apt-get -y upgrade    Install other software   apt-get -y install subversion tmux git curl libncurses5-dev gcc make g++ python-dev unzip dh-autoreconf zlib1g-dev libboost1.55-dev sparsehash openmpi*    Install SPAdes   cd $HOME\nwget http://spades.bioinf.spbau.ru/release3.1.1/SPAdes-3.1.1-Linux.tar.gz\ntar -zxf SPAdes-3.1.1-Linux.tar.gz\ncd SPAdes-3.1.1-Linux\nPATH=$PATH:$(pwd)/bin    Install ABySS   cd $HOME\ngit clone https://github.com/bcgsc/abyss.git\ncd abyss\n./autogen.sh\n./configure --enable-maxk=128 --prefix=/usr/local/ --with-mpi=/usr/lib/openmpi/\nmake -j4\nmake all install  -   Install a script for assembly evaluation.   git clone https://github.com/lexnederbragt/sequencetools.git\ncd sequencetools/\nPATH=$PATH:$(pwd)   Download and unpack the data   cd /mnt\nwget https://s3.amazonaws.com/gen711/ecoli_data.tar.gz\ntar -zxf ecoli_data.tar.gz   Assembly. Try this with different data combos (with mate pair data, without, with minION data and without, etc). Remember to name your assemblies something different using the  -o  flag. Spades has a built-in error correction tool (remove  --only-assembler ). Does 'double error correction seem to make a difference?'.   mkdir /mnt/spades\ncd /mnt/spades\n\nspades.py -t 8 -m 15 --only-assembler --mp1-rf -k 127 \\\n--pe1-1 /mnt/ecoli_pe.1.fq \\\n--pe1-2 /mnt/ecoli_pe.2.fq \\\n--mp1-1 /mnt/nextera.1.fq \\\n--mp1-2 /mnt/nextera.2.fq \\\n--pacbio /mnt/minion.data.fasta \\\n-o Ecoli_all_data    Evaluate Assemblies   abyss-fac Ecoli_all_data/scaffolds.fasta\n\n#take a closer look.\n\nassemblathon_stats.pl Ecoli_all_data/scaffolds.fasta   Assembling with ABySS (optional)   mkdir /mnt/abyss\ncd /mnt/abyss\n\nabyss-pe np=8 k=127 name=ecoli lib='pe1' mp='mp1' long='minion' \\\npe1='/mnt/ecoli_pe.1.fq /mnt/ecoli_pe.2.fq' \\\nmp1='/mnt/nextera.1.fq /mnt/nextera.2.fq' \\\nminion='/mnt/minion.data.fasta' mp1_l=30", 
            "title": "Lab 10: Bacterial Genome Assembly"
        }, 
        {
            "location": "/lab_lessons/Lab1_blast/", 
            "text": "Lab 1: BLAST\n\n\nDuring this lab, we will acquaint ourselves with the\u00a0the software package BLAST. Your objectives are:\n\n\n\n\n\n\nFamiliarize yourself with the software, how to execute it, how to visualize results.\n\n\n\n\n\n\nRegarding your dataset, tell me how some of these genes are related to their homologous copies.\n\n\n\n\n\n\nStep 1: Launch and AMI. You will need to navigate to aws.amazon.com to do this. Follow the instructions on the \nAmazon Resources\n page if you cannot remember how to launch an instance.  For this exercise,\u00a0we will use a\u00a0c4.xlarge instance, which has 4 cores and costs 21 cents per hour to rent.\n\n\n# This is a comment, anything that follow the hashtag symbol is a comment.\n\n# replace the `x's` in the below command with your Public IP address\n\nssh -i ~/Downloads/your.pem ubuntu@xx.xx.xx.x\n\n\n\nThe machine you are using is Linux Ubuntu: Ubuntu is an operating system you can use (I do) on your laptop or desktop. One of the nice things about this OS is the ability to update the software, easily.\u00a0 The command \nsudo apt-get update\n checks a server for updates to existing software. \napt-get\n is like the Mac app store for those of you that are Mac/iPhone users.\n\n\nsudo apt-get update\n\n\n\nThe upgrade command actually installs any of the required updates.\n\n\nsudo apt-get -y upgrade\n\n\n\nOK, what are these commands?\u00a0 \nsudo\n is the command that tells the computer that we have admin privileges. Try running the commands without the sudo -- it will complain that you don't have admin privileges or something like that. \nCareful here, using sudo means that you can do something really bad to your own computer -- like delete everything\n, so use with caution. It's not a big worry when using AWS, as this is a virtual machine- fixing your worst mistake is as easy as just terminating the instance and restarting.\n\n\nSo now that we have updates the software, lets see how to add new software. Same basic command, but instead of the \nupdate\n or \nupgrade\n command, we're using \ninstall\n. EASY!!\n\n\nsudo apt-get -y install build-essential git curl gcc make g++ python-dev unzip\n\n\n\nok, for this lab we are going to use BLAST, which is available as a package entitled \nncbi-blast+\n\n\n# replace the ??? with the appropriate command that will install blast.\n\nsudo apt-get -y install ???\n\n\n\nto get a feel for the different options, type \nblastp -help\n. Which type of blast does this correspond to? Look at the help info for blastp and tblastx. There are A LOT of options, most of which you will be able to safely ignore. One of the challenges of bioinformatics is knowing about the options and which ones you can safely ignore, and which ones are important.\n\n\nChallenge...\n\n\nYou have just returned from South America, where you captured a rare - never been seen before - desert mouse. You are interested in knowing how it survives, and start by trying to identify the Sodium transport genes, in particular a gene called Scn5a (https://en.wikipedia.org/wiki/Nav1.5). You've just had the animals's transcriptome sequenced, and are about to begin the search!! You \ncould\n use web blast like you've done in the past, but there are 20166 sequences that you need to search and that would take a loooooooong time.\n\n\nDownload the data here:\n\n\ncurl -LO https://s3.amazonaws.com/macmanes_share/transcripts.fasta\n\n\n\nYou know that the model organism, the lab mouse, has an excellent genome and you decide to use it to help identify the sodium transport genes in the new animal. Download the data:\n\n\ncurl -LO ftp://ftp.ensembl.org/pub/release-85/fasta/mus_musculus/cdna/Mus_musculus.GRCm38.cdna.all.fa.gz\ngzip -d Mus_musculus.GRCm38.cdna.all.fa.gz\n\n\n\nOK, let's do some blasting..\n\n\n1st step is to make the blast database\n\n\nmakeblastdb -in Mus_musculus.GRCm38.cdna.all.fa -out mus -dbtype nucl\n\n\n\nNow blast.. your results should be saved in a file called \nblast.out\n\n\nblastn -db mus -max_target_seqs 1 -query transcripts.fasta \\\n-outfmt '6 qseqid evalue stitle' -evalue 1e-10 -num_threads 4 -out blast.out\n\n\n\nlook at \nblast.out\n. the 1st column is the ID of the desert mouse transcript, the 2nd column is the e-value (a statistic related to how good the match between query and reference sequences is - smaller numbers better. The 3rd column is the Mouse transcript match descriptor)\n\n\nless -S blast.out\n\n\n\nOh crap.. there are 16501 lines in that file, how are we going to find the Scn5a gene that we are looking for?? Meet \ngrep\n.\n\n\ngrep -i SCN5A blast.out\n\n\n\nAs an additional exercise, see if you sequenced your favorite gene in the new animals. Navigate to http://useast.ensembl.org/Mus_musculus/Info/Index to find the appropriate gene codes. For example..\n\n\ngrep -i \"solute carrier\" blast.out\ngrep -i aquaporin blast.out\ngrep -i Rbp2 blast.out", 
            "title": "Lab1 blast"
        }, 
        {
            "location": "/lab_lessons/Lab1_blast/#lab-1-blast", 
            "text": "During this lab, we will acquaint ourselves with the\u00a0the software package BLAST. Your objectives are:    Familiarize yourself with the software, how to execute it, how to visualize results.    Regarding your dataset, tell me how some of these genes are related to their homologous copies.    Step 1: Launch and AMI. You will need to navigate to aws.amazon.com to do this. Follow the instructions on the  Amazon Resources  page if you cannot remember how to launch an instance.  For this exercise,\u00a0we will use a\u00a0c4.xlarge instance, which has 4 cores and costs 21 cents per hour to rent.  # This is a comment, anything that follow the hashtag symbol is a comment.\n\n# replace the `x's` in the below command with your Public IP address\n\nssh -i ~/Downloads/your.pem ubuntu@xx.xx.xx.x  The machine you are using is Linux Ubuntu: Ubuntu is an operating system you can use (I do) on your laptop or desktop. One of the nice things about this OS is the ability to update the software, easily.\u00a0 The command  sudo apt-get update  checks a server for updates to existing software.  apt-get  is like the Mac app store for those of you that are Mac/iPhone users.  sudo apt-get update  The upgrade command actually installs any of the required updates.  sudo apt-get -y upgrade  OK, what are these commands?\u00a0  sudo  is the command that tells the computer that we have admin privileges. Try running the commands without the sudo -- it will complain that you don't have admin privileges or something like that.  Careful here, using sudo means that you can do something really bad to your own computer -- like delete everything , so use with caution. It's not a big worry when using AWS, as this is a virtual machine- fixing your worst mistake is as easy as just terminating the instance and restarting.  So now that we have updates the software, lets see how to add new software. Same basic command, but instead of the  update  or  upgrade  command, we're using  install . EASY!!  sudo apt-get -y install build-essential git curl gcc make g++ python-dev unzip  ok, for this lab we are going to use BLAST, which is available as a package entitled  ncbi-blast+  # replace the ??? with the appropriate command that will install blast.\n\nsudo apt-get -y install ???  to get a feel for the different options, type  blastp -help . Which type of blast does this correspond to? Look at the help info for blastp and tblastx. There are A LOT of options, most of which you will be able to safely ignore. One of the challenges of bioinformatics is knowing about the options and which ones you can safely ignore, and which ones are important.", 
            "title": "Lab 1: BLAST"
        }, 
        {
            "location": "/lab_lessons/Lab1_blast/#challenge", 
            "text": "You have just returned from South America, where you captured a rare - never been seen before - desert mouse. You are interested in knowing how it survives, and start by trying to identify the Sodium transport genes, in particular a gene called Scn5a (https://en.wikipedia.org/wiki/Nav1.5). You've just had the animals's transcriptome sequenced, and are about to begin the search!! You  could  use web blast like you've done in the past, but there are 20166 sequences that you need to search and that would take a loooooooong time.  Download the data here:  curl -LO https://s3.amazonaws.com/macmanes_share/transcripts.fasta  You know that the model organism, the lab mouse, has an excellent genome and you decide to use it to help identify the sodium transport genes in the new animal. Download the data:  curl -LO ftp://ftp.ensembl.org/pub/release-85/fasta/mus_musculus/cdna/Mus_musculus.GRCm38.cdna.all.fa.gz\ngzip -d Mus_musculus.GRCm38.cdna.all.fa.gz", 
            "title": "Challenge..."
        }, 
        {
            "location": "/lab_lessons/Lab1_blast/#ok-lets-do-some-blasting", 
            "text": "1st step is to make the blast database  makeblastdb -in Mus_musculus.GRCm38.cdna.all.fa -out mus -dbtype nucl  Now blast.. your results should be saved in a file called  blast.out  blastn -db mus -max_target_seqs 1 -query transcripts.fasta \\\n-outfmt '6 qseqid evalue stitle' -evalue 1e-10 -num_threads 4 -out blast.out  look at  blast.out . the 1st column is the ID of the desert mouse transcript, the 2nd column is the e-value (a statistic related to how good the match between query and reference sequences is - smaller numbers better. The 3rd column is the Mouse transcript match descriptor)  less -S blast.out  Oh crap.. there are 16501 lines in that file, how are we going to find the Scn5a gene that we are looking for?? Meet  grep .  grep -i SCN5A blast.out  As an additional exercise, see if you sequenced your favorite gene in the new animals. Navigate to http://useast.ensembl.org/Mus_musculus/Info/Index to find the appropriate gene codes. For example..  grep -i \"solute carrier\" blast.out\ngrep -i aquaporin blast.out\ngrep -i Rbp2 blast.out", 
            "title": "OK, let's do some blasting.."
        }, 
        {
            "location": "/lab_lessons/Lab3_hmmer/", 
            "text": "Lab3 : HMMER\n\n\nDuring this lab, we will acquaint ourselves with the\u00a0the software package HMMER. Your objectives are:\n\n\n-\n\n\n\n\n\n\nFamiliarize yourself with the software, how to execute it, how to visualize results.\n\n\n\n\n\n\nRegarding your dataset. Characterize a few conserved domains.\n\n\n\n\n\n\nThe HMMER manual\u00a0\nftp://selab.janelia.org/pub/software/hmmer3/3.1b1/Userguide.pdf\n\n\nThe HMMER webpage:\u00a0\nhttp://hmmer.janelia.org/\n\n\n\n\n\n\nStep 1: Launch and AMI. For this exercise,\u00a0we will use a\u00a0c3.xlarge instance. Remember to change the permission of your key code \nchmod 400 ~/Downloads/your.pem\n (change your.pem to whatever you named it)\n\n\n\n\nssh -i ~/Downloads/your.pem ubuntu@ec2-???-???-???-???.compute-1.amazonaws.com\n\n\n\n\n\nsudo bash\napt-get update\napt-get -y upgrade\napt-get -y install tmux git curl gcc make g++ python-dev unzip default-jre\n\n\n\n-\n\n\n\n\nOk, for this lab we are going to use HMMER\n\n\n\n\n-\n\n\ncd $HOME\nwget http://selab.janelia.org/software/hmmer3/3.1b1/hmmer-3.1b1-linux-intel-x86_64.tar.gz\ntar -zxf hmmer-3.1b1-linux-intel-x86_64.tar.gz\ncd hmmer-3.1b1-linux-intel-x86_64/\n./configure\nmake \n make all install\nmake check\n\n\n\n\n\n-\n\n\n\n\nYou will download\u00a0one of the 5 different datasets (use the same dataset). Do you remember how to use the \nwget\n and \ngzip\n commands from last week? Also, download Swissprot and Pfam-A\n\n\n\n\n-\n\n\ncd /mnt\n\n#download your dataset\n\ndataset1= https://www.dropbox.com/s/srfk4o2bh1qmq6l/dataset1.fa.gz\ndataset2= https://www.dropbox.com/s/977n0ibznzuor22/dataset2.fa.gz\ndataset3= https://www.dropbox.com/s/8s2h7sm6xtoky6q/dataset3.fa.gz\ndataset4= https://www.dropbox.com/s/qth3mjrianb48a6/dataset4.fa.gz\ndataset5= https://www.dropbox.com/s/quexoxfh6ttmudo/dataset5.fa.gz\n\n#download the SwissProt database\n\nwget ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz\n\n#download the Pfam-A database\n\nwget ftp://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/Pfam-A.hmm.gz\n\n\n\n\n\nwe are going to run HMMER to identify conserved protein domains. This will take a little while, and we'll use \ntmux\n to allow us to do this in the background, and continue to work on other things.\n\n\n\n\ngzip\u00a0-d *gz\ntmux new -s pfam\nhmmpress Pfam-A.hmm #this is analogous to 'makeblastdb'\nhmmscan -E 1e-3 --domtblout dataset.pfam --cpu 4 Pfam-A.hmm dataset1.fa\nctl-b d\ntop -c #see that hmmscan is running..\n\n\n\n\n\nthe neat thing about HMMER is that it can be used as a replacement for blastP or PSI-blast.\n\n\n\n\n#blastp-like HBB-HUMAN is a Hemoglobin B protein sequence.\n\nphmmer --domtblout hbb.phmmer -E 1e-5 \\\n/home/ubuntu/hmmer-3.1b1-linux-intel-x86_64/tutorial/HBB_HUMAN \\\nuniprot_sprot.fasta\n\n#PSI-blast-like\n\njackhmmer --domtblout hbb.jackhammer -E 1e-5 \\\n/home/ubuntu/hmmer-3.1b1-linux-intel-x86_64/tutorial/HBB_HUMAN \\\nuniprot_sprot.fasta\n\n#you can look at the results using `more hmm.phmmer` or `more hmm.jackhmmer`. Try blasting a few of the results using the BLAST web interface.\n\n\n\n\n\nNow let's look at the Pfam results. This analyses may still be running, but we can look at it while it's still in progress.\n\n\n\n\nmore dataset.pfam\n#There are a bunch of columns in this table - what do they mean?\n\n#Try to extract all the hits to a specific domain. Google a few domains (column 1) to see if any seem interesting.\n\n#for instance, find all occurrences of ABC_tran\ngrep ABC_tran dataset.pfam\n\n#use grep to count the number of matches. Copy this number down.\n\ngrep -c ABC_tran dataset.pfam\n\n#Find all the contigs that have a ABC_tran domain.\n\ngrep ABC_tran dataset.pfam | awk '{print $4}' | sort | uniq\n\n\n\n\n\nJust for fun, check on the Pfam search to see what it is doing... \n\n\n\n\ntmux attach -t pfam\nctl-b d", 
            "title": "Lab3 hmmer"
        }, 
        {
            "location": "/lab_lessons/Lab4_fastq/", 
            "text": "--\n\n\nDuring this lab, we will acquaint ourselves with the\u00a0the software packages FastQC and JellyFish. Your objectives are:\n\n\n-\n\n\n\n\n\n\nFamiliarize yourself with the software, how to execute it, how to visualize results.\n\n\n\n\n\n\nRegarding your dataset. Characterize\u00a0sequence quality.\n\n\n\n\n\n\nThe FastQC manual:\u00a0\nhttp://www.bioinformatics.babraham.ac.uk/projects/download.html#fastqc\n\n\nThe JellyFish manual:\u00a0\nftp://ftp.genome.umd.edu/pub/jellyfish/JellyfishUserGuide.pdf\n\n\n\n\n\n\nStep 1: Launch and AMI. For this exercise,\u00a0we will use a\u00a0c3.xlarge instance. Remember to change the permission of your key code \nchmod 400 ~/Downloads/????.pem\n (change ????.pem to whatever you named it)\n\n\n\n\nssh -i ~/Downloads/?????.pem ubuntu@ec2-???-???-???-???.compute-1.amazonaws.com\n\n\n\n\n\n\n\nUpdate Software\n\n\n\n\nsudo bash\napt-get update\n\n\n\n\n\n\n\nInstall updates\n\n\n\n\napt-get -y upgrade\n\n\n\n\n\n\n\nInstall other software\n\n\n\n\napt-get -y install tmux git curl gcc make g++ python-dev unzip default-jre\n\n\n\n-\n\n\n\n\nOk, for this lab we are going to use\u00a0FastQC. There is a version available on apt-get, but it is an old version and we want to make sure that we have the most updated version.. \nMake sure you know what each of these commands does, rather than blindly copying and pasting..\u00a0\n\n\n\n\n-\n\n\ncd $HOME\nwget http://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.2.zip\nunzip fastqc_v0.11.2.zip\ncd FastQC/\nchmod +x fastqc\nPATH=$PATH:$(pwd)\n\n\n\n\n\n\n\nDownload data, and uncompress them.. What does the \n-cd\n flag mean WRT gzip??\n\n\n\n\n-\n\n\ncd /mnt\nwget https://s3.amazonaws.com/gen711/Pero360B.1.fastq.gz\nwget https://s3.amazonaws.com/gen711/Pero360B.2.fastq.gz\ngzip -cd /mnt/Pero360B.1.fastq.gz \n /mnt/Pero360B.1.fastq \n\ngzip -cd /mnt/Pero360B.2.fastq.gz \n /mnt/Pero360B.2.fastq \n\n\n\n\n\n\n\n\nInstall Fastool, a neat and fast tool used for fastQ --\n  fastA\n\n\n\n\n-\n\n\ncd $HOME\ngit clone https://github.com/fstrozzi/Fastool.git\ncd Fastool/\nmake\nPATH=$PATH:$(pwd)\n\n\n\n\n\n\n\nUse Fastool to convert from fastQ to fastA\n\n\n\n\n-\n\n\ncd /mnt\nfastool --to-fasta Pero360B.1.fastq \n Pero360B.1.fasta \n\nfastool --to-fasta Pero360B.2.fastq \n Pero360B.2.fasta \n\n\n\n\n\n\n\n\nWhile Fastool is working, lets install JellyFish..\u00a0\nAgain, make sure you know what each of these commands does, rather than just\u00a0copying and pasting..\u00a0\n\n\n\n\n-\n\n\ncd $HOME\nwget ftp://ftp.genome.umd.edu/pub/jellyfish/jellyfish-2.1.3.tar.gz\ntar -zxf jellyfish-2.1.3.tar.gz\ncd jellyfish-2.1.3/\n./configure\nmake\nPATH=$PATH:$(pwd)/bin\n\n\n\n\n\n\n\nRun FastQC. Make sure to look at the manual to see what the different outputs mean.\n\n\n\n\ncd /mnt\nfastqc -t 4 Pero360B.1.fastq Pero360B.2.fastq\n\n\n\n\n\n\n\nRun Jellyfish. Make sure to look at the manual.\n\n\n\n\ncd /mnt\nmkdir jelly\ncd jelly\njellyfish count -F2 -m 25 -s 200M -t 4 -C ../Pero360B.1.fasta ../Pero360B.2.fasta\njellyfish histo mer_counts.jf \n Pero360B.histo\nhead -50 Pero360B.histo\n\n\n\n\n\n\n\nOpen up a new terminal window using the buttons command-t\n\n\n\n\nscp -i ~/Downloads/????.pem ubuntu@ec2-??-???-???-??.compute-1.amazonaws.com:/mnt/*zip ~/Downloads/\nscp -i ~/Downloads/????.pem ubuntu@ec2-??-???-???-??.compute-1.amazonaws.com:/mnt/jelly/*histo ~/Downloads/\n\n\n\n\n\nNow, on your MAC, find the files you just downloaded - for the zip files - double click and that should unzip them.. Click on the \nhtml\n file, which will open up your browser. Look at the results. Try to figure out what each plot means.\n\n\n\n\n-\n\n\n\n\nNow look at the \n.histo\n file, which is a kmer distribution. I want you to plot the distribution using R and RStudio.\n\n\n\n\n-\n\n\n\n\nOPEN RSTUDIO\n\n\n\n\n#Import Data\nhisto \n- read.table(\"~/Downloads/Pero360B.histo\", quote=\"\\\"\")\nhead(histo)\n\n#Plot\nplot(histo$V2 ~ histo$V1, type='h')\n\n#That one sucks, but what does it tell you about the kmer distribution?\n\n#Maybe this one is better?\nplot(histo$V2 ~ histo$V1, type='h', xlim=c(0,100))\n\n#Better. what is xlim? Maybe we can still improve?\n\nplot(histo$V2 ~ histo$V1, type='h', xlim=c(0,500), ylim=c(0,1000000))\n\n#Final plot\n\nplot(histo$V2 ~ histo$V1, type='h', xlim=c(0,500), ylim=c(0,1000000),\n        col='blue', frame.plot=F, xlab='25-mer frequency', ylab='Count',\n        main='Kmer distribution in brain sample before quality trimming')\n\n\n\n\n\nDone?", 
            "title": "Lab4 fastq"
        }, 
        {
            "location": "/lab_lessons/Lab5_mapping/", 
            "text": "Lab 5: Read Mapping\n\n\nDuring this lab, we will acquaint ourselves with de novo transcriptome assembly using Trinity. You will:\n\n\n\n\n\n\nInstall software and download data\n\n\n\n\n\n\nUse sra-toolkit to extract fastQ reads\n\n\n\n\n\n\nMap reads to dataset\n\n\n\n\n\n\nlook at mapping quality\n\n\n\n\n\n\nThe BWA manual: http://bio-bwa.sourceforge.net/\n\n\nStep 1: Launch and AMI. For this exercise, we will use a c4.2xlarge machine. Add 100Gb storage.\n\n\nUpdate Software\n\n\nsudo apt-get update \n sudo apt-get -y upgrade\n\n\n\n\nInstall other software\n\n\nsudo apt-get -y install build-essential git\n\n\n\n\nInstall LinuxBrew\n\n\n\ncd\nwget https://keybase.io/mpapis/key.asc\ngpg --import key.asc\n\\curl -sSL https://get.rvm.io | bash -s stable --ruby\nsource /home/ubuntu/.rvm/scripts/rvm\n\n\nsudo mkdir /home/linuxbrew\nsudo chown $USER:$USER /home/linuxbrew\ngit clone https://github.com/Linuxbrew/brew.git /home/linuxbrew/.linuxbrew\necho 'export PATH=\n/home/linuxbrew/.linuxbrew/bin:$PATH\n' \n ~/.profile\necho 'export MANPATH=\n/home/linuxbrew/.linuxbrew/share/man:$MANPATH\n' \n ~/.profile\necho 'export INFOPATH=\n/home/linuxbrew/.linuxbrew/share/info:$INFOPATH\n' \n ~/.profile\nsource ~/.profile\nbrew tap homebrew/science\nbrew update\nbrew doctor\n\n\n\n\nInstall software\n\n\nbrew install bwa samtools aria2 salmon sratoolkit\n\n\n\n\nDownload data\n\n\nmkdir $HOME/data\ncd $HOME/data\naria2c http://datadryad.org/bitstream/handle/10255/dryad.72141/brain.final.fasta\naria2c ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/SRR/SRR157/SRR1575395/SRR1575395.sra\n\n\n\n\n\ncd $HOME/data\nfastq-dump --split-files --split-spot SRR1575395.sra\n\n\n\n\nMap reads!! (17 minutes). You're mapping to a mouse brain transcriptome reference.\n\n\nmkdir $HOME/mapping\ncd $HOME/mapping\n\n\ntmux new -s mapping\n\n\nbwa index -p index $HOME/data/brain.final.fasta\ntime bwa mem -t8 index $HOME/data/SRR1575395_1.fastq $HOME/data/SRR1575395_2.fastq | samtools view -1 -o brain.bam -@ 4 -T $HOME/data/brain.final.fasta -\n\n\n\n\n\nLook at BAM file. Can you see the columns that we talked about in class?\n\n\n#Take a quick general look.\n\nsamtools view brain.bam | head\n\n\n\n\nlook at mapping stats. Figure out what this means.\n\n\nsamtools flagstat brain.bam\n\n\n\n\nMap with salmon\n\n\nsalmon index --type quasi --threads 8 --index brain.idx -t $HOME/data/brain.final.fasta\nsalmon quant --gcBias --seqBias -p 8 -i brain.idx -l a -1 $HOME/data/SRR1575395_1.fastq -2  $HOME/data/SRR1575395_2.fastq -o salmon_brain\n\n\n\n\nLastly, look at the Salmon output using the command \nmore salmon_brain/quant.sf\n. Pick a random transcript (column 1), and note the number of fragments that map to it (this is in col. 5). Let's see how concordant this number is with the BWA output.\n\n\nTo count the number of reads mapping to a specific transcript, we need to search the BAM/SAM file for the name. Try the code:\n\n\nsamtools view brain.bam | awk '{print $3}' | grep -c NAME_OF_YOUR_TRANSCRIPT\n\n\n\n\nMake sure you know what this command is doing, rather than just copy/pasting..\n\n\nTERMINATE YOUR INSTANCE", 
            "title": "Lab5 mapping"
        }, 
        {
            "location": "/lab_lessons/Lab5_mapping/#lab-5-read-mapping", 
            "text": "During this lab, we will acquaint ourselves with de novo transcriptome assembly using Trinity. You will:    Install software and download data    Use sra-toolkit to extract fastQ reads    Map reads to dataset    look at mapping quality    The BWA manual: http://bio-bwa.sourceforge.net/  Step 1: Launch and AMI. For this exercise, we will use a c4.2xlarge machine. Add 100Gb storage.  Update Software  sudo apt-get update   sudo apt-get -y upgrade  Install other software  sudo apt-get -y install build-essential git  Install LinuxBrew  \ncd\nwget https://keybase.io/mpapis/key.asc\ngpg --import key.asc\n\\curl -sSL https://get.rvm.io | bash -s stable --ruby\nsource /home/ubuntu/.rvm/scripts/rvm\n\n\nsudo mkdir /home/linuxbrew\nsudo chown $USER:$USER /home/linuxbrew\ngit clone https://github.com/Linuxbrew/brew.git /home/linuxbrew/.linuxbrew\necho 'export PATH= /home/linuxbrew/.linuxbrew/bin:$PATH '   ~/.profile\necho 'export MANPATH= /home/linuxbrew/.linuxbrew/share/man:$MANPATH '   ~/.profile\necho 'export INFOPATH= /home/linuxbrew/.linuxbrew/share/info:$INFOPATH '   ~/.profile\nsource ~/.profile\nbrew tap homebrew/science\nbrew update\nbrew doctor  Install software  brew install bwa samtools aria2 salmon sratoolkit  Download data  mkdir $HOME/data\ncd $HOME/data\naria2c http://datadryad.org/bitstream/handle/10255/dryad.72141/brain.final.fasta\naria2c ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/SRR/SRR157/SRR1575395/SRR1575395.sra  cd $HOME/data\nfastq-dump --split-files --split-spot SRR1575395.sra  Map reads!! (17 minutes). You're mapping to a mouse brain transcriptome reference.  mkdir $HOME/mapping\ncd $HOME/mapping\n\n\ntmux new -s mapping\n\n\nbwa index -p index $HOME/data/brain.final.fasta\ntime bwa mem -t8 index $HOME/data/SRR1575395_1.fastq $HOME/data/SRR1575395_2.fastq | samtools view -1 -o brain.bam -@ 4 -T $HOME/data/brain.final.fasta -  Look at BAM file. Can you see the columns that we talked about in class?  #Take a quick general look.\n\nsamtools view brain.bam | head  look at mapping stats. Figure out what this means.  samtools flagstat brain.bam  Map with salmon  salmon index --type quasi --threads 8 --index brain.idx -t $HOME/data/brain.final.fasta\nsalmon quant --gcBias --seqBias -p 8 -i brain.idx -l a -1 $HOME/data/SRR1575395_1.fastq -2  $HOME/data/SRR1575395_2.fastq -o salmon_brain  Lastly, look at the Salmon output using the command  more salmon_brain/quant.sf . Pick a random transcript (column 1), and note the number of fragments that map to it (this is in col. 5). Let's see how concordant this number is with the BWA output.  To count the number of reads mapping to a specific transcript, we need to search the BAM/SAM file for the name. Try the code:  samtools view brain.bam | awk '{print $3}' | grep -c NAME_OF_YOUR_TRANSCRIPT  Make sure you know what this command is doing, rather than just copy/pasting..", 
            "title": "Lab 5: Read Mapping"
        }, 
        {
            "location": "/lab_lessons/Lab5_mapping/#terminate-your-instance", 
            "text": "", 
            "title": "TERMINATE YOUR INSTANCE"
        }, 
        {
            "location": "/lab_lessons/Lab5_trimming/", 
            "text": "Lab 5: Trimming\n\n\n\n\nDuring this lab, we will acquaint ourselves with the\u00a0the software packages FastQC and JellyFish. Your objectives are:\n\n\n-\n\n\n\n\n\n\nFamiliarize yourself with the software, how to execute it, how to visualize results.\n\n\n\n\n\n\nRegarding your dataset. Characterize\u00a0sequence quality.\n\n\n\n\n\n\nThe FastQC manual:\u00a0\nhttp://www.bioinformatics.babraham.ac.uk/projects/download.html#fastqc\n\n\nThe JellyFish manual:\u00a0\nftp://ftp.genome.umd.edu/pub/jellyfish/JellyfishUserGuide.pdf\n\n\n\n\n\n\nStep 1: Launch and AMI. For this exercise,\u00a0we will use a\u00a0c3.xlarge instance. Remember to change the permission of your key code \nchmod 400 ~/Downloads/????.pem\n (change ????.pem to whatever you named it)\n\n\n\n\nssh -i ~/Downloads/?????.pem ubuntu@ec2-???-???-???-???.compute-1.amazonaws.com\n\n\n\n\n\n\n\nUpdate Software\n\n\n\n\nsudo bash\napt-get update\n\n\n\n\n\n\n\nInstall updates\n\n\n\n\napt-get -y upgrade\n\n\n\n\n\n\n\nInstall other software\n\n\n\n\napt-get -y install tmux git curl gcc make g++ python-dev unzip default-jre\n\n\n\n-\n\n\n\n\nOk, for this lab we are going to use\u00a0FastQC. There is a version available on apt-get, but it is an old version and we want to make sure that we have the most updated version.. \nMake sure you know what each of these commands does, rather than blindly copying and pasting..\u00a0\n\n\n\n\n-\n\n\ncd $HOME\nwget http://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.2.zip\nunzip fastqc_v0.11.2.zip\ncd FastQC/\nchmod +x fastqc\nPATH=$PATH:$(pwd)\n\n\n\n\n\n\n\nInstall Trimmomatic\n\n\n\n\ncd $HOME\nwget http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.32.zip\nunzip Trimmomatic-0.32.zip\ncd Trimmomatic-0.32\nchmod +x trimmomatic-0.32.jar\n\n\n\n\n\n\n\nInstall Jellyfish\n\n\n\n\ncd $HOME\nwget ftp://ftp.genome.umd.edu/pub/jellyfish/jellyfish-2.1.3.tar.gz\ntar -zxf jellyfish-2.1.3.tar.gz\ncd jellyfish-2.1.3/\n./configure\nmake\nPATH=$PATH:$(pwd)/bin\n\n\n\n\n\n\n\nDownload data. For this lab, we'll be using only 1 sequencing file.\n\n\n\n\n-\n\n\ncd /mnt\nwget https://s3.amazonaws.com/gen711/Pero360B.2.fastq.gz\n\n\n\n\n\n\n\nDo 3 different trimming levels between 2 and 40. This one is trimming at a Phred score of 30 (BAD!!!) When you run your commands, you'll need to change the numbers in \nLEADING:30\n \nTRAILING:30\n \nSLIDINGWINDOW:4:30\n and \nPero360B.trim.Phred30.fastq\n to whatever trimming level you want to use.\n\n\n\n\nmkdir /mnt/trimming\ncd /mnt/trimming\n\n#paste the below lines together as 1 command\n\njava -Xmx10g -jar $HOME/Trimmomatic-0.32/trimmomatic-0.32.jar SE \\\n-threads 4 \\\n../Pero360B.2.fastq.gz \\\nPero360B.trim.Phred30.fastq \\\nILLUMINACLIP:$HOME/Trimmomatic-0.32/adapters/TruSeq3-PE.fa:2:30:10 \\\nSLIDINGWINDOW:4:30 \\\nLEADING:30 \\\nTRAILING:30 \\\nMINLEN:25\n\n\n\n\n\n\n\nAfter Trimmomatic is done, Run FastQC. You'll have to change the numbers to match the levels you trimmed at.\n\n\n\n\ncd /mnt\nfastqc -t 4 Pero360B.2.fastq.gz\nfastqc -t 4 trimming/Pero360B.trim.Phred2.fastq\nfastqc -t 4 trimming/Pero360B.trim.Phred15.fastq\nfastqc -t 4 trimming/Pero360B.trim.Phred30.fastq\n\n\n\n\n\n\n\nRun Jellyfish.\n\n\n\n\nmkdir /mnt/jelly\ncd /mnt/jelly\n\n# You'll have to run these commands 4 separate times -\n# once for each different trimmed dataset, and once for the raw dataset.\n# Change the names of the input and output files..\n\njellyfish count -m 25 -s 200M -t 4 -C -o trim30.jf ../trimming/Pero360B.trim.Phred30.fastq\njellyfish histo trim30.jf -o trim30.histo\n\n\n\n\n\n\n\nOpen up a new terminal window using the buttons command-t\n\n\n\n\nscp -i ~/Downloads/????.pem ubuntu@ec2-??-???-???-??.compute-1.amazonaws.com:/mnt/*zip ~/Downloads/\nscp -i ~/Downloads/????.pem ubuntu@ec2-??-???-???-??.compute-1.amazonaws.com:/mnt/jelly/*histo ~/Downloads/\n\n\n\n\n\nNow, on your MAC, find the files you just downloaded - for the zip files - double click and that should unzip them.. Click on the \nhtml\n file, which will open up your browser. Look at the results. Try to figure out what each plot means.\n\n\n\n\n-\n\n\n\n\nNow look at the \n.histo\n file, which is a kmer distribution. I want you to plot the distribution using R and RStudio.\n\n\n\n\n-\n\n\n\n\nOPEN RSTUDIO\n\n\n\n\n#Import all 3 histogram datasets: this is the code for importing 1 of them..\n\ntrim2 \n- read.table(\"~/Downloads/trim2.histo\", quote=\"\\\"\")\n\n#Plot: Make sure and change the names to match what you import.\n#What does this plot show you??\n\nbarplot(c(trim2$V2[1],trim15$V2[1],trim30$V2[1]),\n    names=c('Phred2', 'Phred15', 'Phred30'),\n    main='Number of unique kmers')\n\n# plot differences between non-unique kmers\n\nplot(trim2$V2[2:30] - trim30$V2[2:30], type='l',\n    xlim=c(2,20), xaxs=\"i\", yaxs=\"i\", frame.plot=F,\n    ylim=c(0,2000000), col='red', xlab='kmer frequency',\n    lwd=4, ylab='count',\n    main='Diff in 25mer counts of freq 2 to 20 \\n Phred2 vs. Phred30')\n\n\n\n\n\nLook at the FastQC plots across the different trimming levels. Anything surprising?\n\n\nWhat do the analyses of kmer counts tell you?", 
            "title": "Lab5 trimming"
        }, 
        {
            "location": "/lab_lessons/Lab5_trimming/#lab-5-trimming", 
            "text": "During this lab, we will acquaint ourselves with the\u00a0the software packages FastQC and JellyFish. Your objectives are:  -    Familiarize yourself with the software, how to execute it, how to visualize results.    Regarding your dataset. Characterize\u00a0sequence quality.    The FastQC manual:\u00a0 http://www.bioinformatics.babraham.ac.uk/projects/download.html#fastqc  The JellyFish manual:\u00a0 ftp://ftp.genome.umd.edu/pub/jellyfish/JellyfishUserGuide.pdf    Step 1: Launch and AMI. For this exercise,\u00a0we will use a\u00a0c3.xlarge instance. Remember to change the permission of your key code  chmod 400 ~/Downloads/????.pem  (change ????.pem to whatever you named it)   ssh -i ~/Downloads/?????.pem ubuntu@ec2-???-???-???-???.compute-1.amazonaws.com    Update Software   sudo bash\napt-get update    Install updates   apt-get -y upgrade    Install other software   apt-get -y install tmux git curl gcc make g++ python-dev unzip default-jre  -   Ok, for this lab we are going to use\u00a0FastQC. There is a version available on apt-get, but it is an old version and we want to make sure that we have the most updated version..  Make sure you know what each of these commands does, rather than blindly copying and pasting..\u00a0   -  cd $HOME\nwget http://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.2.zip\nunzip fastqc_v0.11.2.zip\ncd FastQC/\nchmod +x fastqc\nPATH=$PATH:$(pwd)    Install Trimmomatic   cd $HOME\nwget http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.32.zip\nunzip Trimmomatic-0.32.zip\ncd Trimmomatic-0.32\nchmod +x trimmomatic-0.32.jar    Install Jellyfish   cd $HOME\nwget ftp://ftp.genome.umd.edu/pub/jellyfish/jellyfish-2.1.3.tar.gz\ntar -zxf jellyfish-2.1.3.tar.gz\ncd jellyfish-2.1.3/\n./configure\nmake\nPATH=$PATH:$(pwd)/bin    Download data. For this lab, we'll be using only 1 sequencing file.   -  cd /mnt\nwget https://s3.amazonaws.com/gen711/Pero360B.2.fastq.gz    Do 3 different trimming levels between 2 and 40. This one is trimming at a Phred score of 30 (BAD!!!) When you run your commands, you'll need to change the numbers in  LEADING:30   TRAILING:30   SLIDINGWINDOW:4:30  and  Pero360B.trim.Phred30.fastq  to whatever trimming level you want to use.   mkdir /mnt/trimming\ncd /mnt/trimming\n\n#paste the below lines together as 1 command\n\njava -Xmx10g -jar $HOME/Trimmomatic-0.32/trimmomatic-0.32.jar SE \\\n-threads 4 \\\n../Pero360B.2.fastq.gz \\\nPero360B.trim.Phred30.fastq \\\nILLUMINACLIP:$HOME/Trimmomatic-0.32/adapters/TruSeq3-PE.fa:2:30:10 \\\nSLIDINGWINDOW:4:30 \\\nLEADING:30 \\\nTRAILING:30 \\\nMINLEN:25    After Trimmomatic is done, Run FastQC. You'll have to change the numbers to match the levels you trimmed at.   cd /mnt\nfastqc -t 4 Pero360B.2.fastq.gz\nfastqc -t 4 trimming/Pero360B.trim.Phred2.fastq\nfastqc -t 4 trimming/Pero360B.trim.Phred15.fastq\nfastqc -t 4 trimming/Pero360B.trim.Phred30.fastq    Run Jellyfish.   mkdir /mnt/jelly\ncd /mnt/jelly\n\n# You'll have to run these commands 4 separate times -\n# once for each different trimmed dataset, and once for the raw dataset.\n# Change the names of the input and output files..\n\njellyfish count -m 25 -s 200M -t 4 -C -o trim30.jf ../trimming/Pero360B.trim.Phred30.fastq\njellyfish histo trim30.jf -o trim30.histo    Open up a new terminal window using the buttons command-t   scp -i ~/Downloads/????.pem ubuntu@ec2-??-???-???-??.compute-1.amazonaws.com:/mnt/*zip ~/Downloads/\nscp -i ~/Downloads/????.pem ubuntu@ec2-??-???-???-??.compute-1.amazonaws.com:/mnt/jelly/*histo ~/Downloads/   Now, on your MAC, find the files you just downloaded - for the zip files - double click and that should unzip them.. Click on the  html  file, which will open up your browser. Look at the results. Try to figure out what each plot means.   -   Now look at the  .histo  file, which is a kmer distribution. I want you to plot the distribution using R and RStudio.   -   OPEN RSTUDIO   #Import all 3 histogram datasets: this is the code for importing 1 of them..\n\ntrim2  - read.table(\"~/Downloads/trim2.histo\", quote=\"\\\"\")\n\n#Plot: Make sure and change the names to match what you import.\n#What does this plot show you??\n\nbarplot(c(trim2$V2[1],trim15$V2[1],trim30$V2[1]),\n    names=c('Phred2', 'Phred15', 'Phred30'),\n    main='Number of unique kmers')\n\n# plot differences between non-unique kmers\n\nplot(trim2$V2[2:30] - trim30$V2[2:30], type='l',\n    xlim=c(2,20), xaxs=\"i\", yaxs=\"i\", frame.plot=F,\n    ylim=c(0,2000000), col='red', xlab='kmer frequency',\n    lwd=4, ylab='count',\n    main='Diff in 25mer counts of freq 2 to 20 \\n Phred2 vs. Phred30')   Look at the FastQC plots across the different trimming levels. Anything surprising?  What do the analyses of kmer counts tell you?", 
            "title": "Lab 5: Trimming"
        }, 
        {
            "location": "/lab_lessons/Lab6_khmer/", 
            "text": "Lab 6: khmer\n\n\n\n\nDuring this lab, we will acquaint ourselves with\u00a0digital normalization. You will:\n\n\n\n\n\n\nInstall software and download data\n\n\n\n\n\n\nQuality and adapter trim data sets.\n\n\n\n\n\n\nApply digital normalization to the dataset.\n\n\n\n\n\n\nCount and compare kmers and kmer distributions in the normalized and un-normalized dataset.\n\n\n\n\n\n\nPlot in RStudio.\n\n\n\n\n\n\n-\n\n\nThe JellyFish manual:\u00a0\nftp://ftp.genome.umd.edu/pub/jellyfish/JellyfishUserGuide.pdf\n\n\nThe Khmer manual:\u00a0\nhttp://khmer.readthedocs.org/en/v1.1/\n\n\n\n\n\n\nStep 1: Launch and AMI. For this exercise,\u00a0we will use a\u00a0c3.xlarge instance. Remember to change the permission of your key code \nchmod 400 ~/Downloads/????.pem\n (change ????.pem to whatever you named it)\n\n\n\n\nssh -i ~/Downloads/?????.pem ubuntu@ec2-???-???-???-???.compute-1.amazonaws.com\n\n\n\n\n\n\n\nUpdate Software\n\n\n\n\nsudo bash\napt-get update\n\n\n\n\n\n\n\nInstall updates\n\n\n\n\napt-get -y upgrade\n\n\n\n\n\n\n\nInstall other software\n\n\n\n\napt-get -y install tmux git curl gcc make g++ python-dev unzip default-jre python-pip zlib1g-dev\n\n\n\n\n\n\n\nInstall Trimmomatic\n\n\n\n\ncd $HOME\nwget http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.32.zip\nunzip Trimmomatic-0.32.zip\ncd Trimmomatic-0.32\nchmod +x trimmomatic-0.32.jar\n\n\n\n\n\n\n\nInstall Jellyfish\n\n\n\n\ncd $HOME\nwget ftp://ftp.genome.umd.edu/pub/jellyfish/jellyfish-2.1.3.tar.gz\ntar -zxf jellyfish-2.1.3.tar.gz\ncd jellyfish-2.1.3/\n./configure\nmake -j4\nPATH=$PATH:$(pwd)/bin\n\n\n\n\n\n\n\nInstall Khmer\n\n\n\n\ncd $HOME\npip install screed pysam\ngit clone https://github.com/ged-lab/khmer.git\ncd khmer\nmake -j4\nmake install\nPATH=$PATH:$(pwd)/scripts\n\n\n\n\n\n\n\nDownload data. For this lab, we'll be using a smaller dataset that consists of 10million paired end reads.\n\n\n\n\n-\n\n\ncd /mnt\nwget https://s3.amazonaws.com/gen711/raw.10M.SRR797058_1.fastq.gz\nwget https://s3.amazonaws.com/gen711/raw.10M.SRR797058_2.fastq.gz\n\n\n\n\n\n\n\nTrim low quality bases and adapters from dataset. These files will form the basis of all out subsequent analyses.\n\n\n\n\n-\n\n\nmkdir /mnt/trimming\ncd /mnt/trimming\n\n#paste the below lines together as 1 command\n\njava -Xmx10g -jar $HOME/Trimmomatic-0.32/trimmomatic-0.32.jar PE \\\n-threads 4 -baseout P2.trimmed.fastQ \\\n/mnt/raw.10M.SRR797058_1.fastq.gz \\\n/mnt/raw.10M.SRR797058_2.fastq.gz \\\nILLUMINACLIP:$HOME/Trimmomatic-0.32/adapters/TruSeq3-PE.fa:2:30:10 \\\nSLIDINGWINDOW:4:2 \\\nLEADING:2 \\\nTRAILING:2 \\\nMINLEN:25\n\n\n\n\n\n\n\nRun Jellyfish on the un-normalized dataset.\n\n\n\n\nmkdir /mnt/jelly\ncd /mnt/jelly\n\njellyfish count -m 25 -F2 -s 700M -t 4 -C -o trimmed.jf /mnt/trimming/P2.trimmed.fastQ_1P /mnt/trimming/P2.trimmed.fastQ_2P\njellyfish histo trimmed.jf -o trimmed.histo\n\n\n\n\n\n\n\nRun Khmer\n\n\n\n\nmkdir /mnt/khmer\ncd /mnt/khmer\ninterleave-reads.py /mnt/trimming/P2.trimmed.fastQ_1P /mnt/trimming/P2.trimmed.fastQ_2P -o interleaved.fq\nnormalize-by-median.py -p -x 15e8 -k 25 -C 50 --out khmer_normalized.fq interleaved.fq\n\n\n\n\n\n\n\nRun Khmer on the normalized dataset.\n\n\n\n\ncd /mnt/jelly\n\njellyfish count -m 25 -s 700M -t 4 -C -o khmer.jf /mnt/khmer/khmer_normalized.fq\njellyfish histo khmer.jf -o khmer.histo\n\n\n\n\n\nOpen up a new terminal window using the buttons command-t\n\n\n\n\nscp -i ~/Downloads/????.pem ubuntu@ec2-??-???-???-??.compute-1.amazonaws.com:/mnt/jelly/*histo ~/Downloads/\n\n\n\n\n\nNow, on your MAC, find the files you just downloaded - for the zip files - double click and that should unzip them.. Click on the \nhtml\n file, which will open up your browser. Look at the results. Try to figure out what each plot means.\n\n\n\n\n-\n\n\n\n\nNow look at the \n.histo\n file, which is a kmer distribution. I want you to plot the distribution using R and RStudio.\n\n\n\n\n-\n\n\n\n\nOPEN RSTUDIO\n\n\n\n\n#Import all 2 histogram datasets: this is the code for importing 1 of them..\n\nkhmer \n- read.table(\"~/Downloads/khmer.histo\", quote=\"\\\"\")\ntrim \n- read.table(\"~/Downloads/trimmed.histo\", quote=\"\\\"\")\n\n#What does this plot show you??\n\nbarplot(c(trim$V2[1],khmer$V2[1]),\n    names=c('Non-normalized', 'C50 Normalized'),\n    main='Number of unique kmers')\n\n# plot differences between non-unique kmers\n\nplot(khmer$V2[10:300] - trim$V2[10:300], type='l',\n    xlim=c(10,300), xaxs=\"i\", yaxs=\"i\", frame.plot=F,\n    ylim=c(-10000,60000), col='red', xlab='kmer frequency',\n    lwd=4, ylab='count',\n    main='Diff in 25mer counts of \\n normalized vs. un-normalized datasets')\nabline(h=0)\n\n\n\n\n\n\n\n-\n\n\n-\n\n\n\n\nWhat do the analyses of kmer counts tell you?", 
            "title": "Lab6 khmer"
        }, 
        {
            "location": "/lab_lessons/Lab6_khmer/#lab-6-khmer", 
            "text": "During this lab, we will acquaint ourselves with\u00a0digital normalization. You will:    Install software and download data    Quality and adapter trim data sets.    Apply digital normalization to the dataset.    Count and compare kmers and kmer distributions in the normalized and un-normalized dataset.    Plot in RStudio.    -  The JellyFish manual:\u00a0 ftp://ftp.genome.umd.edu/pub/jellyfish/JellyfishUserGuide.pdf  The Khmer manual:\u00a0 http://khmer.readthedocs.org/en/v1.1/    Step 1: Launch and AMI. For this exercise,\u00a0we will use a\u00a0c3.xlarge instance. Remember to change the permission of your key code  chmod 400 ~/Downloads/????.pem  (change ????.pem to whatever you named it)   ssh -i ~/Downloads/?????.pem ubuntu@ec2-???-???-???-???.compute-1.amazonaws.com    Update Software   sudo bash\napt-get update    Install updates   apt-get -y upgrade    Install other software   apt-get -y install tmux git curl gcc make g++ python-dev unzip default-jre python-pip zlib1g-dev    Install Trimmomatic   cd $HOME\nwget http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.32.zip\nunzip Trimmomatic-0.32.zip\ncd Trimmomatic-0.32\nchmod +x trimmomatic-0.32.jar    Install Jellyfish   cd $HOME\nwget ftp://ftp.genome.umd.edu/pub/jellyfish/jellyfish-2.1.3.tar.gz\ntar -zxf jellyfish-2.1.3.tar.gz\ncd jellyfish-2.1.3/\n./configure\nmake -j4\nPATH=$PATH:$(pwd)/bin    Install Khmer   cd $HOME\npip install screed pysam\ngit clone https://github.com/ged-lab/khmer.git\ncd khmer\nmake -j4\nmake install\nPATH=$PATH:$(pwd)/scripts    Download data. For this lab, we'll be using a smaller dataset that consists of 10million paired end reads.   -  cd /mnt\nwget https://s3.amazonaws.com/gen711/raw.10M.SRR797058_1.fastq.gz\nwget https://s3.amazonaws.com/gen711/raw.10M.SRR797058_2.fastq.gz    Trim low quality bases and adapters from dataset. These files will form the basis of all out subsequent analyses.   -  mkdir /mnt/trimming\ncd /mnt/trimming\n\n#paste the below lines together as 1 command\n\njava -Xmx10g -jar $HOME/Trimmomatic-0.32/trimmomatic-0.32.jar PE \\\n-threads 4 -baseout P2.trimmed.fastQ \\\n/mnt/raw.10M.SRR797058_1.fastq.gz \\\n/mnt/raw.10M.SRR797058_2.fastq.gz \\\nILLUMINACLIP:$HOME/Trimmomatic-0.32/adapters/TruSeq3-PE.fa:2:30:10 \\\nSLIDINGWINDOW:4:2 \\\nLEADING:2 \\\nTRAILING:2 \\\nMINLEN:25    Run Jellyfish on the un-normalized dataset.   mkdir /mnt/jelly\ncd /mnt/jelly\n\njellyfish count -m 25 -F2 -s 700M -t 4 -C -o trimmed.jf /mnt/trimming/P2.trimmed.fastQ_1P /mnt/trimming/P2.trimmed.fastQ_2P\njellyfish histo trimmed.jf -o trimmed.histo    Run Khmer   mkdir /mnt/khmer\ncd /mnt/khmer\ninterleave-reads.py /mnt/trimming/P2.trimmed.fastQ_1P /mnt/trimming/P2.trimmed.fastQ_2P -o interleaved.fq\nnormalize-by-median.py -p -x 15e8 -k 25 -C 50 --out khmer_normalized.fq interleaved.fq    Run Khmer on the normalized dataset.   cd /mnt/jelly\n\njellyfish count -m 25 -s 700M -t 4 -C -o khmer.jf /mnt/khmer/khmer_normalized.fq\njellyfish histo khmer.jf -o khmer.histo   Open up a new terminal window using the buttons command-t   scp -i ~/Downloads/????.pem ubuntu@ec2-??-???-???-??.compute-1.amazonaws.com:/mnt/jelly/*histo ~/Downloads/   Now, on your MAC, find the files you just downloaded - for the zip files - double click and that should unzip them.. Click on the  html  file, which will open up your browser. Look at the results. Try to figure out what each plot means.   -   Now look at the  .histo  file, which is a kmer distribution. I want you to plot the distribution using R and RStudio.   -   OPEN RSTUDIO   #Import all 2 histogram datasets: this is the code for importing 1 of them..\n\nkhmer  - read.table(\"~/Downloads/khmer.histo\", quote=\"\\\"\")\ntrim  - read.table(\"~/Downloads/trimmed.histo\", quote=\"\\\"\")\n\n#What does this plot show you??\n\nbarplot(c(trim$V2[1],khmer$V2[1]),\n    names=c('Non-normalized', 'C50 Normalized'),\n    main='Number of unique kmers')\n\n# plot differences between non-unique kmers\n\nplot(khmer$V2[10:300] - trim$V2[10:300], type='l',\n    xlim=c(10,300), xaxs=\"i\", yaxs=\"i\", frame.plot=F,\n    ylim=c(-10000,60000), col='red', xlab='kmer frequency',\n    lwd=4, ylab='count',\n    main='Diff in 25mer counts of \\n normalized vs. un-normalized datasets')\nabline(h=0)    -  -   What do the analyses of kmer counts tell you?", 
            "title": "Lab 6: khmer"
        }, 
        {
            "location": "/lab_lessons/Lab7_transcriptome_assembly/", 
            "text": "Lab 7: Transcriptome assembly\n\n\n--\n\n\nDuring this lab, we will acquaint ourselves with\u00a0de novo transcriptome assembly using Trinity. You will:\n\n\n\n\n\n\nInstall software and download data\n\n\n\n\n\n\nError correct, quality and adapter trim data sets.\n\n\n\n\n\n\nApply digital normalization to the dataset.\n\n\n\n\n\n\nTrinity assembly\n\n\n\n\n\n\nBecause the above steps will take a few hours, I am providing you with 2 datasets: one is the 10 million read dataset you used last week. The other is that same 10M read dataset that I have error corrected, quality/adapter trimmed, normalized, and subsampled to 0.5 million reads (I did this so that the assembly could be done in a reasonable amount of time). Especially for the people who are going to do \nde novo\n transcriptome projects, and students who will use something like this in their own research, that it is probably worth going through the whole pipeline at some point.\n\n\n\n\n\n\n-\n\n\nThe JellyFish manual:\u00a0\nftp://ftp.genome.umd.edu/pub/jellyfish/JellyfishUserGuide.pdf\n\n\nThe Khmer manual:\u00a0\nhttp://khmer.readthedocs.org/en/v1.1/\n\n\nTrinity reference material: \nhttp://trinityrnaseq.sourceforge.net/\n\n\n\n\n\n\nStep 1: Launch and AMI. For this exercise,\u00a0we will use a\u00a0\nm3.2xlarge\n\u00a0(note different instance type). Remember to change the permission of your key code \nchmod 400 ~/Downloads/????.pem\n (change ????.pem to whatever you named it)\n\n\n\n\nssh -i ~/Downloads/?????.pem ubuntu@ec2-???-???-???-???.compute-1.amazonaws.com\n\n\n\n\n\n\n\nUpdate Software\n\n\n\n\nsudo bash\napt-get update\n\n\n\n\n\n\n\nInstall updates\n\n\n\n\napt-get -y upgrade\n\n\n\n\n\n\n\nInstall other software\n\n\n\n\napt-get -y install subversion tmux git curl bowtie libncurses5-dev samtools gcc make g++ python-dev unzip dh-autoreconf default-jre python-pip zlib1g-dev\n\n\n\n\n\n\n\nInstall Lighter, software for error correction.\n\n\n\n\ncd $HOME\ngit clone https://github.com/mourisl/Lighter.git\nmake -j8\nPATH=$PATH:$(pwd)/scripts\n\n\n\n\n\n\n\nInstall Trimmomatic\n\n\n\n\ncd $HOME\nwget http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.32.zip\nunzip Trimmomatic-0.32.zip\ncd Trimmomatic-0.32\nchmod +x trimmomatic-0.32.jar\n\n\n\n\n\n\n\nInstall Trinity\n\n\n\n\ncd $HOME\nsvn checkout svn://svn.code.sf.net/p/trinityrnaseq/code/trunk trinityrnaseq-code\ncd trinityrnaseq-code\nmake -j8\nPATH=$PATH:$(pwd)\n\n\n\n\n\n\n\nInstall Khmer\n\n\n\n\ncd $HOME\npip install screed pysam\ngit clone https://github.com/ged-lab/khmer.git\ncd khmer\nmake -j8\nmake install\nPATH=$PATH:$(pwd)/scripts\n\n\n\n\n\n\n\nDownload data. For this lab, these data are to be used by people wanting to do the whole pipeline. Most people will want the other dataset I link to below here..\n\n\n\n\ncd /mnt\nwget https://s3.amazonaws.com/gen711/raw.10M.SRR797058_1.fastq.gz\nwget https://s3.amazonaws.com/gen711/raw.10M.SRR797058_2.fastq.gz\n\n\n\n\n\nAlternatively, you can download the pre-corrected, trimmed, normalized datasets. Sadly, I had to subsample this dataset severely (to 500,000 reads) so that we could assemble it in a lab period...\n\n\n\n\ncd /mnt\nwget https://www.dropbox.com/s/eo3wrx6lvngq3ja/ec.P2.C25.left.fq.gz\nwget https://www.dropbox.com/s/eycchg3m2my2ag2/ec.P2.C25.right.fq.gz\n\n\n\n\n\n\n\nError correct (do this step if you are working with the raw data only). Note you will have to uncompress the data if you are doing these steps. I chose the software 'lighter' because if is 1. probably good and 2. it is fast! It is written by Ben Langmead, the author of several of the powerpoint lectures I posted last week.\n\n\n\n\nmkdir /mnt/ec\ncd /mnt/ec\nlighter -r /mnt/raw.10M.SRR797058_1.fastq -r /mnt/raw.10M.SRR797058_2.fastq -t 8 -k 25 100000000 .1\n\n\n\n\n\n\n\nTrim\u00a0(do this step if you are working with the raw data only)\n\n\n\n\nmkdir /mnt/trimming\ncd /mnt/trimming\n\n#paste the below lines together as 1 command\n\njava -Xmx10g -jar $HOME/Trimmomatic-0.32/trimmomatic-0.32.jar PE \\\n-threads 8 -baseout ec.P2trim.fastQ \\\n/mnt/ec/raw.10M.SRR797058_1.cor.fq \\\n/mnt/ec/raw.10M.SRR797058_2.cor.fq \\\nILLUMINACLIP:$HOME/Trimmomatic-0.32/adapters/TruSeq3-PE.fa:2:30:10 \\\nSLIDINGWINDOW:4:2 \\\nLEADING:2 \\\nTRAILING:2 \\\nMINLEN:25\n\n\n\n\n\n\n\nRun Khmer\u00a0(do this step if you are working with the raw data only)\n\n\n\n\nmkdir /mnt/khmer\ncd /mnt/khmer\ninterleave-reads.py /mnt/trimming/ec.P2trim.fastQ_1P /mnt/trimming/ec.P2trim.fastQ_2P -o interleaved.fq\nnormalize-by-median.py -p -x 15e8 -k 25 -C 25 --out khmer_normalized.fq interleaved.fq\nsplit-paired-reads.py khmer_normalized.fq\n\n\n\n\n\n\n\nRun Trinity - everybody do this. If you are running with the raw data, you'll have to change the names of the input files. Note that I am using \n--min_kmer_cov 2\n in the command below. This is only so that you can get through the assembly in a short amount of time. DO NOT USE THIS OPTION IN 'REAL LIFE' AS IT WILL MAKE YOUR ASSEMBLY WORSE!!! This should take ~30 minutes, so use this time to talk to your group members, or whatever else..\n\n\n\n\nmkdir /mnt/trinity\ncd /mnt/trinity\nTrinity --seqType fq --JM 20G --min_kmer_cov 2 \\\n--left /mnt/ec.P2.C25.left.fq \\\n--right /mnt/ec.P2.C25.right.fq \\\n--CPU 8 --output ec.P2trim.C25 --group_pairs_distance 999 --inchworm_cpu 8\n\n\n\n\n\n\n\nGenerate Length Based stats from your assembly. What do these mean?\n\n\n\n\n$HOME/trinityrnaseq-code/util/TrinityStats.pl ec.P2trim.C25/Trinity.fasta\n\n\n\n\n\nlets looks for coding sequences. Before we can do this, we need to install a Perl module using the cpan command.\n\n\n\n\ncpan URI::Escape\n$HOME/trinityrnaseq-code/trinity-plugins/TransDecoder_r20140704/TransDecoder --CPU 8 -t ec.P2trim.C25/Trinity.fasta\n\n\n\n\n\nThis will take a few minutes. Once done, you will have a file of amino acid sequences, and coding sequences. Look at how many coding sequences you found, and how many were complete (have a start and stop codon) vs. fragmented in one way or another. What do these numbers mean?? What would you hope these numbers look like. What does \ngrep -c\n do?\n\n\n\n\n$HOME/trinityrnaseq-code/util/TrinityStats.pl Trinity.fasta.transdecoder.pep\ngrep -c complete Trinity.fasta.transdecoder.pep\ngrep -c internal Trinity.fasta.transdecoder.pep\ngrep -c 5prime Trinity.fasta.transdecoder.pep\ngrep -c 3prime Trinity.fasta.transdecoder.pep", 
            "title": "Lab7 transcriptome assembly"
        }, 
        {
            "location": "/lab_lessons/Lab7_transcriptome_assembly/#lab-7-transcriptome-assembly", 
            "text": "--  During this lab, we will acquaint ourselves with\u00a0de novo transcriptome assembly using Trinity. You will:    Install software and download data    Error correct, quality and adapter trim data sets.    Apply digital normalization to the dataset.    Trinity assembly    Because the above steps will take a few hours, I am providing you with 2 datasets: one is the 10 million read dataset you used last week. The other is that same 10M read dataset that I have error corrected, quality/adapter trimmed, normalized, and subsampled to 0.5 million reads (I did this so that the assembly could be done in a reasonable amount of time). Especially for the people who are going to do  de novo  transcriptome projects, and students who will use something like this in their own research, that it is probably worth going through the whole pipeline at some point.    -  The JellyFish manual:\u00a0 ftp://ftp.genome.umd.edu/pub/jellyfish/JellyfishUserGuide.pdf  The Khmer manual:\u00a0 http://khmer.readthedocs.org/en/v1.1/  Trinity reference material:  http://trinityrnaseq.sourceforge.net/    Step 1: Launch and AMI. For this exercise,\u00a0we will use a\u00a0 m3.2xlarge \u00a0(note different instance type). Remember to change the permission of your key code  chmod 400 ~/Downloads/????.pem  (change ????.pem to whatever you named it)   ssh -i ~/Downloads/?????.pem ubuntu@ec2-???-???-???-???.compute-1.amazonaws.com    Update Software   sudo bash\napt-get update    Install updates   apt-get -y upgrade    Install other software   apt-get -y install subversion tmux git curl bowtie libncurses5-dev samtools gcc make g++ python-dev unzip dh-autoreconf default-jre python-pip zlib1g-dev    Install Lighter, software for error correction.   cd $HOME\ngit clone https://github.com/mourisl/Lighter.git\nmake -j8\nPATH=$PATH:$(pwd)/scripts    Install Trimmomatic   cd $HOME\nwget http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.32.zip\nunzip Trimmomatic-0.32.zip\ncd Trimmomatic-0.32\nchmod +x trimmomatic-0.32.jar    Install Trinity   cd $HOME\nsvn checkout svn://svn.code.sf.net/p/trinityrnaseq/code/trunk trinityrnaseq-code\ncd trinityrnaseq-code\nmake -j8\nPATH=$PATH:$(pwd)    Install Khmer   cd $HOME\npip install screed pysam\ngit clone https://github.com/ged-lab/khmer.git\ncd khmer\nmake -j8\nmake install\nPATH=$PATH:$(pwd)/scripts    Download data. For this lab, these data are to be used by people wanting to do the whole pipeline. Most people will want the other dataset I link to below here..   cd /mnt\nwget https://s3.amazonaws.com/gen711/raw.10M.SRR797058_1.fastq.gz\nwget https://s3.amazonaws.com/gen711/raw.10M.SRR797058_2.fastq.gz   Alternatively, you can download the pre-corrected, trimmed, normalized datasets. Sadly, I had to subsample this dataset severely (to 500,000 reads) so that we could assemble it in a lab period...   cd /mnt\nwget https://www.dropbox.com/s/eo3wrx6lvngq3ja/ec.P2.C25.left.fq.gz\nwget https://www.dropbox.com/s/eycchg3m2my2ag2/ec.P2.C25.right.fq.gz    Error correct (do this step if you are working with the raw data only). Note you will have to uncompress the data if you are doing these steps. I chose the software 'lighter' because if is 1. probably good and 2. it is fast! It is written by Ben Langmead, the author of several of the powerpoint lectures I posted last week.   mkdir /mnt/ec\ncd /mnt/ec\nlighter -r /mnt/raw.10M.SRR797058_1.fastq -r /mnt/raw.10M.SRR797058_2.fastq -t 8 -k 25 100000000 .1    Trim\u00a0(do this step if you are working with the raw data only)   mkdir /mnt/trimming\ncd /mnt/trimming\n\n#paste the below lines together as 1 command\n\njava -Xmx10g -jar $HOME/Trimmomatic-0.32/trimmomatic-0.32.jar PE \\\n-threads 8 -baseout ec.P2trim.fastQ \\\n/mnt/ec/raw.10M.SRR797058_1.cor.fq \\\n/mnt/ec/raw.10M.SRR797058_2.cor.fq \\\nILLUMINACLIP:$HOME/Trimmomatic-0.32/adapters/TruSeq3-PE.fa:2:30:10 \\\nSLIDINGWINDOW:4:2 \\\nLEADING:2 \\\nTRAILING:2 \\\nMINLEN:25    Run Khmer\u00a0(do this step if you are working with the raw data only)   mkdir /mnt/khmer\ncd /mnt/khmer\ninterleave-reads.py /mnt/trimming/ec.P2trim.fastQ_1P /mnt/trimming/ec.P2trim.fastQ_2P -o interleaved.fq\nnormalize-by-median.py -p -x 15e8 -k 25 -C 25 --out khmer_normalized.fq interleaved.fq\nsplit-paired-reads.py khmer_normalized.fq    Run Trinity - everybody do this. If you are running with the raw data, you'll have to change the names of the input files. Note that I am using  --min_kmer_cov 2  in the command below. This is only so that you can get through the assembly in a short amount of time. DO NOT USE THIS OPTION IN 'REAL LIFE' AS IT WILL MAKE YOUR ASSEMBLY WORSE!!! This should take ~30 minutes, so use this time to talk to your group members, or whatever else..   mkdir /mnt/trinity\ncd /mnt/trinity\nTrinity --seqType fq --JM 20G --min_kmer_cov 2 \\\n--left /mnt/ec.P2.C25.left.fq \\\n--right /mnt/ec.P2.C25.right.fq \\\n--CPU 8 --output ec.P2trim.C25 --group_pairs_distance 999 --inchworm_cpu 8    Generate Length Based stats from your assembly. What do these mean?   $HOME/trinityrnaseq-code/util/TrinityStats.pl ec.P2trim.C25/Trinity.fasta   lets looks for coding sequences. Before we can do this, we need to install a Perl module using the cpan command.   cpan URI::Escape\n$HOME/trinityrnaseq-code/trinity-plugins/TransDecoder_r20140704/TransDecoder --CPU 8 -t ec.P2trim.C25/Trinity.fasta   This will take a few minutes. Once done, you will have a file of amino acid sequences, and coding sequences. Look at how many coding sequences you found, and how many were complete (have a start and stop codon) vs. fragmented in one way or another. What do these numbers mean?? What would you hope these numbers look like. What does  grep -c  do?   $HOME/trinityrnaseq-code/util/TrinityStats.pl Trinity.fasta.transdecoder.pep\ngrep -c complete Trinity.fasta.transdecoder.pep\ngrep -c internal Trinity.fasta.transdecoder.pep\ngrep -c 5prime Trinity.fasta.transdecoder.pep\ngrep -c 3prime Trinity.fasta.transdecoder.pep", 
            "title": "Lab 7: Transcriptome assembly"
        }, 
        {
            "location": "/lab_lessons/Lab8_mapping/", 
            "text": "Lab 8: Read Mapping\n\n\n\n\nDuring this lab, we will acquaint ourselves with\u00a0de novo transcriptome assembly using Trinity. You will:\n\n\n\n\n\n\nInstall software and download data\n\n\n\n\n\n\nUse sra-toolkit to extract fastQ reads\n\n\n\n\n\n\nMap reads to dataset\n\n\n\n\n\n\nlook at mapping quality\n\n\n\n\n\n\n-\n\n\nThe BWA manual: http://bio-bwa.sourceforge.net/\u00a0\n\n\nFlag info: \nhttp://broadinstitute.github.io/picard/explain-flags.html\n\n\n\n\n\n\nStep 1: Launch and AMI. For this exercise,\u00a0we will use a \nc3.2xlarge\n (yet another instance type). Remember to change the permission of your key code \nchmod 400 ~/Downloads/????.pem\n (change ????.pem to whatever you named it)\n\n\n\n\nssh -i ~/Downloads/?????.pem ubuntu@ec2-???-???-???-???.compute-1.amazonaws.com\n\n\n\n\n\n\n\nUpdate Software\n\n\n\n\nsudo bash\napt-get update\n\n\n\n\n\n\n\nInstall updates\n\n\n\n\napt-get -y upgrade\n\n\n\n\n\n\n\nInstall other software\n\n\n\n\napt-get -y install subversion tmux git curl samtools gcc make g++ python-dev unzip dh-autoreconf default-jre zlib1g-dev\n\n\n\n\n\ncd $HOME\ngit clone https://github.com/lh3/bwa.git\ncd bwa\nmake -j4\nPATH=$PATH:$(pwd)\n\n\n\n\n\ncd $HOME\nwget http://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.4.2/sratoolkit.2.4.2-ubuntu64.tar.gz\ntar -zxf sratoolkit.2.4.2-ubuntu64.tar.gz\nPATH=$PATH:/home/ubuntu/sratoolkit.2.4.2-ubuntu64/bin\n\n\n\n\n\nDownload data\n\n\n\n\nmkdir /mnt/data\ncd /mnt/data\nwget http://datadryad.org/bitstream/handle/10255/dryad.72141/brain.final.fasta\nwget ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/SRR/SRR157/SRR1575395/SRR1575395.sra\n\n\n\n\n\nConvert SRA format into fastQ (takes a few minutes)\n\n\n\n\ncd /mnt/data\nfastq-dump --split-files --split-spot SRR1575395.sra\n\n\n\n\n\nMap reads!! (20 minutes)\n\n\n\n\nmkdir /mnt/mapping\ncd /mnt/mapping\ntmux new -s mapping\nbwa index -p index /mnt/data/brain.final.fasta\nbwa mem -t8 index /mnt/data/SRR1575395_1.fastq /mnt/data/SRR1575395_2.fastq \n brain.sam\n\n\n\n\n\nLook at SAM file.\n\n\n\n\n#Take a quick general look.\n\nhead brain.sam\ntail brain.sam\n\n#Count how many reads in fastq files. `grep -c` counts the number of occurances of the pattern, which in this case is `^@`. I am looking for lines that begin with (specified by `^`) the @ character.\n\ngrep -c ^@ ../data/SRR1575395_1.fastq ../data/SRR1575395_2.fastq\n\n#count number of reads mapping with Flag 65/67. The 1st part of this command `awk`, pulls out the second column of the files, and counts everthing that has either 65 or 67. What do these flags correspond to?\n\nawk '{print $2}' brain.sam | grep ^6 | grep -c '65\\|67'\n\n#why do we need the `grep ^6` thing in there... try `awk '{print $2}' brain.sam | grep '65\\|67' | wc -l`\n\n#what about this??\n\nawk '{print $2}' brain.sam | grep '^65\\|^67' | wc -l\n\n\n\n\n\nCan you pull out the number of mismatches targeting the NM tag in column 12?\n\n\n\n\n#I'm giving you the last bit of the awk code. You have to figure out the 1st awk command and the 1st grep command. This will send the number of mismatches to a file `mismatches.txt`. Can you download it to your usb or HD and plot the results, find the mean number of mismatches, etc??\n\nawk | grep | awk -F \":\" '{print $3}' \n mismatches.txt", 
            "title": "Lab8 mapping"
        }, 
        {
            "location": "/lab_lessons/Lab8_mapping/#lab-8-read-mapping", 
            "text": "During this lab, we will acquaint ourselves with\u00a0de novo transcriptome assembly using Trinity. You will:    Install software and download data    Use sra-toolkit to extract fastQ reads    Map reads to dataset    look at mapping quality    -  The BWA manual: http://bio-bwa.sourceforge.net/\u00a0  Flag info:  http://broadinstitute.github.io/picard/explain-flags.html    Step 1: Launch and AMI. For this exercise,\u00a0we will use a  c3.2xlarge  (yet another instance type). Remember to change the permission of your key code  chmod 400 ~/Downloads/????.pem  (change ????.pem to whatever you named it)   ssh -i ~/Downloads/?????.pem ubuntu@ec2-???-???-???-???.compute-1.amazonaws.com    Update Software   sudo bash\napt-get update    Install updates   apt-get -y upgrade    Install other software   apt-get -y install subversion tmux git curl samtools gcc make g++ python-dev unzip dh-autoreconf default-jre zlib1g-dev   cd $HOME\ngit clone https://github.com/lh3/bwa.git\ncd bwa\nmake -j4\nPATH=$PATH:$(pwd)   cd $HOME\nwget http://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.4.2/sratoolkit.2.4.2-ubuntu64.tar.gz\ntar -zxf sratoolkit.2.4.2-ubuntu64.tar.gz\nPATH=$PATH:/home/ubuntu/sratoolkit.2.4.2-ubuntu64/bin   Download data   mkdir /mnt/data\ncd /mnt/data\nwget http://datadryad.org/bitstream/handle/10255/dryad.72141/brain.final.fasta\nwget ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/SRR/SRR157/SRR1575395/SRR1575395.sra   Convert SRA format into fastQ (takes a few minutes)   cd /mnt/data\nfastq-dump --split-files --split-spot SRR1575395.sra   Map reads!! (20 minutes)   mkdir /mnt/mapping\ncd /mnt/mapping\ntmux new -s mapping\nbwa index -p index /mnt/data/brain.final.fasta\nbwa mem -t8 index /mnt/data/SRR1575395_1.fastq /mnt/data/SRR1575395_2.fastq   brain.sam   Look at SAM file.   #Take a quick general look.\n\nhead brain.sam\ntail brain.sam\n\n#Count how many reads in fastq files. `grep -c` counts the number of occurances of the pattern, which in this case is `^@`. I am looking for lines that begin with (specified by `^`) the @ character.\n\ngrep -c ^@ ../data/SRR1575395_1.fastq ../data/SRR1575395_2.fastq\n\n#count number of reads mapping with Flag 65/67. The 1st part of this command `awk`, pulls out the second column of the files, and counts everthing that has either 65 or 67. What do these flags correspond to?\n\nawk '{print $2}' brain.sam | grep ^6 | grep -c '65\\|67'\n\n#why do we need the `grep ^6` thing in there... try `awk '{print $2}' brain.sam | grep '65\\|67' | wc -l`\n\n#what about this??\n\nawk '{print $2}' brain.sam | grep '^65\\|^67' | wc -l   Can you pull out the number of mismatches targeting the NM tag in column 12?   #I'm giving you the last bit of the awk code. You have to figure out the 1st awk command and the 1st grep command. This will send the number of mismatches to a file `mismatches.txt`. Can you download it to your usb or HD and plot the results, find the mean number of mismatches, etc??\n\nawk | grep | awk -F \":\" '{print $3}'   mismatches.txt", 
            "title": "Lab 8: Read Mapping"
        }, 
        {
            "location": "/lab_lessons/Lab9_euk.genome.assembly/", 
            "text": "Lab 9: Genome assembly\n\n\n\n\nDuring this lab, we will acquaint ourselves with\u00a0Genome Assembly using SPAdes. We will assembly the genome of Plasmodium falciparum. The data are taken from this paper: http://www.nature.com/ncomms/2014/140909/ncomms5754/full/ncomms5754.html?WT.ec_id=JA-NCOMMS-20140919.\nAs it stands right now, I think that you will do all the preprocessing steps this week, then the assembly next. Once you have done all the steps, \ngzip\n compress the files and download them to your USB drive, or the MAC HD. I can provide you with these files next week if issues arise. \n\n\n\n\n\n\nInstall software and download data\n\n\n\n\n\n\nError correct, quality and adapter trim data sets.\n\n\n\n\n\n\n(next week) Assemble\n\n\n\n\n\n\n-\n\n\nThe SPAdes manuscript: http://www.ncbi.nlm.nih.gov/pubmed/22506599\nThe SPAdes manual: http://spades.bioinf.spbau.ru/release3.1.1/manual.html\nSPAdes website: http://bioinf.spbau.ru/spades\n\n\n\n\nStep 1: Launch and AMI. For this exercise,\u00a0we will use a\u00a0\nc3.8xlarge\n\u00a0(note different instance type). Remember to change the permission of your key code \nchmod 400 ~/Downloads/????.pem\n (change ????.pem to whatever you named it)\n\n\n\n\nssh -i ~/Downloads/?????.pem ubuntu@ec2-???-???-???-???.compute-1.amazonaws.com\n\n\n\n\n\n\n\nUpdate Software\n\n\n\n\nsudo bash\napt-get update\n\n\n\n\n\n\n\nInstall updates\n\n\n\n\napt-get -y upgrade\n\n\n\n\n\n\n\nInstall other software\n\n\n\n\napt-get -y install subversion tmux git curl bowtie libncurses5-dev samtools gcc make g++ python-dev unzip dh-autoreconf default-jre python-pip zlib1g-dev\n\n\n\n\n\n\n\nInstall Lighter, software for error correction.\n\n\n\n\ncd $HOME\ngit clone https://github.com/mourisl/Lighter.git\ncd Lighter\nmake -j8\nPATH=$PATH:$(pwd)\n\n\n\n\n\n\n\nInstall Trimmomatic\n\n\n\n\ncd $HOME\nwget http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.32.zip\nunzip Trimmomatic-0.32.zip\ncd Trimmomatic-0.32\nchmod +x trimmomatic-0.32.jar\n\n\n\n\n\n\n\nInstall SRAtoolkit\n\n\n\n\ncd $HOME\nwget http://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.4.2/sratoolkit.2.4.2-ubuntu64.tar.gz\ntar -zxf sratoolkit.2.4.2-ubuntu64.tar.gz\nPATH=$PATH:/home/ubuntu/sratoolkit.2.4.2-ubuntu64/bin\n\n\n\n\n\n\n\nInstall SPAdes\n\n\n\n\nwget http://spades.bioinf.spbau.ru/release3.1.1/SPAdes-3.1.1-Linux.tar.gz\ntar -zxf SPAdes-3.1.1-Linux.tar.gz\ncd SPAdes-3.1.1-Linux\nPATH=$PATH:$(pwd)/bin\n\n\n\n\n\n\n\nDownload 3.5kb MP library\n\n\n\n\ncd /mnt\nwget ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/ERR/ERR022/ERR022558/ERR022558.sra\n\n\n\n\n\nDownload 10kb MP library\n\n\n\n\ncd /mnt\nwget ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/ERR/ERR022/ERR022557/ERR022557.sra\n\n\n\n\n\nDownload PE library #1\n\n\n\n\ncd /mnt\nwget ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/ERR/ERR019/ERR019273/ERR019273.sra\n\n\n\n\n\n\n\nDownload PE library #2\n\n\n\n\ncd /mnt\nwget ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/ERR/ERR019/ERR019275/ERR019275.sra\n\n\n\n\n\n\n\nExtract fastQ from sra format.\n\n\n\n\ncd /mnt\n\n#this is a basic for loop. Copy is all as 1 line.\n\nfor i in `ls *sra`; do\n    fastq-dump --split-files --split-spot $i;\n    rm $i;\ndone\n\n\n\n\n\n\n\nError Correct Data\n\n\n\n\nmkdir /mnt/ec\ncd /mnt/ec\nlighter -r /mnt/ERR019273_1.fastq -r /mnt/ERR019273_2.fastq -t 32 -k 21 45000000 .1\nlighter -r /mnt/ERR022557_1.fastq -r /mnt/ERR022557_2.fastq -t 32 -k 21 45000000 .1\nlighter -r /mnt/ERR022558_1.fastq -r /mnt/ERR022558_2.fastq -t 32 -k 21 45000000 .1\nlighter -r /mnt/ERR019275_1.fastq -r /mnt/ERR019275_2.fastq -t 32 -k 21 45000000 .1\n\n#remove the raw files.\n\nrm *fastq \n\n\n\n\n\n\ntrim the data:\n\n\n\n\nmkdir /mnt/trim\ncd /mnt/trim\n#paste the below lines together as 1 command\n\njava -Xmx10g -jar $HOME/Trimmomatic-0.32/trimmomatic-0.32.jar PE \\\n-threads 32 -baseout PE_lib1.fq \\\n/mnt/ec/ERR019273_1.cor.fq \\\n/mnt/ec/ERR019273_2.cor.fq \\\nILLUMINACLIP:$HOME/Trimmomatic-0.32/adapters/TruSeq2-PE.fa:2:30:10 \\\nSLIDINGWINDOW:4:2 \\\nLEADING:2 \\\nTRAILING:2 \\\nMINLEN:25\n\n#paste the below lines together as 1 command\n\njava -Xmx10g -jar $HOME/Trimmomatic-0.32/trimmomatic-0.32.jar PE \\\n-threads 32 -baseout PE_lib2.fq \\\n/mnt/ec/ERR019275_1.cor.fq \\\n/mnt/ec/ERR019275_2.cor.fq \\\nILLUMINACLIP:$HOME/Trimmomatic-0.32/adapters/TruSeq2-PE.fa:2:30:10 \\\nSLIDINGWINDOW:4:2 \\\nLEADING:2 \\\nTRAILING:2 \\\nMINLEN:25\n\n#paste the below lines together as 1 command\n\njava -Xmx10g -jar $HOME/Trimmomatic-0.32/trimmomatic-0.32.jar PE \\\n-threads 32 -baseout MP10000.fq \\\n/mnt/ec/ERR022557_1.cor.fq \\\n/mnt/ec/ERR022557_2.cor.fq \\\nILLUMINACLIP:$HOME/Trimmomatic-0.32/adapters/TruSeq2-PE.fa:2:30:10 \\\nSLIDINGWINDOW:4:2 \\\nLEADING:2 \\\nTRAILING:2 \\\nMINLEN:25\n\n#paste the below lines together as 1 command\n\njava -Xmx10g -jar $HOME/Trimmomatic-0.32/trimmomatic-0.32.jar PE \\\n-threads 32 -baseout MP3500.fq \\\n/mnt/ec/ERR022558_1.cor.fq \\\n/mnt/ec/ERR022558_2.cor.fq \\\nILLUMINACLIP:$HOME/Trimmomatic-0.32/adapters/TruSeq2-PE.fa:2:30:10 \\\nSLIDINGWINDOW:4:2 \\\nLEADING:2 \\\nTRAILING:2 \\\nMINLEN:25\n\n\n\n\n\nmkdir /mnt/\n\n#remove the corrected files.\n\nrm *fq\n\n\n\n\n\nAssembly. This you may want to do next week. Alternatively, you can put it in a tmux window and let it run. You'd have to login later however to download the assembled genome.  \n\n\n\n\nmkdir spades\ncd spades\n\nspades.py -t 32 -m 60 \\\n--pe1-1 /mnt/trim/PE_lib1_1P.fq \\\n--pe1-2 /mnt/trim/PE_lib1_2P.fq \\\n--pe2-1 /mnt/trim/PE_lib2_1P.fq \\\n--pe2-2 /mnt/trim/PE_lib2_2P.fq \\\n--mp1-1 /mnt/trim/MP3500_1P.fq \\\n--mp1-2 /mnt/trim/MP3500_2P.fq \\\n--mp2-1 /mnt/trim/MP10000_1P.fq \\\n--mp2-2 /mnt/trim/MP10000_2P.fq \\\n-o Pfal --only-assembler", 
            "title": "Lab9 euk.genome.assembly"
        }, 
        {
            "location": "/lab_lessons/Lab9_euk.genome.assembly/#lab-9-genome-assembly", 
            "text": "During this lab, we will acquaint ourselves with\u00a0Genome Assembly using SPAdes. We will assembly the genome of Plasmodium falciparum. The data are taken from this paper: http://www.nature.com/ncomms/2014/140909/ncomms5754/full/ncomms5754.html?WT.ec_id=JA-NCOMMS-20140919.\nAs it stands right now, I think that you will do all the preprocessing steps this week, then the assembly next. Once you have done all the steps,  gzip  compress the files and download them to your USB drive, or the MAC HD. I can provide you with these files next week if issues arise.     Install software and download data    Error correct, quality and adapter trim data sets.    (next week) Assemble    -  The SPAdes manuscript: http://www.ncbi.nlm.nih.gov/pubmed/22506599\nThe SPAdes manual: http://spades.bioinf.spbau.ru/release3.1.1/manual.html\nSPAdes website: http://bioinf.spbau.ru/spades   Step 1: Launch and AMI. For this exercise,\u00a0we will use a\u00a0 c3.8xlarge \u00a0(note different instance type). Remember to change the permission of your key code  chmod 400 ~/Downloads/????.pem  (change ????.pem to whatever you named it)   ssh -i ~/Downloads/?????.pem ubuntu@ec2-???-???-???-???.compute-1.amazonaws.com    Update Software   sudo bash\napt-get update    Install updates   apt-get -y upgrade    Install other software   apt-get -y install subversion tmux git curl bowtie libncurses5-dev samtools gcc make g++ python-dev unzip dh-autoreconf default-jre python-pip zlib1g-dev    Install Lighter, software for error correction.   cd $HOME\ngit clone https://github.com/mourisl/Lighter.git\ncd Lighter\nmake -j8\nPATH=$PATH:$(pwd)    Install Trimmomatic   cd $HOME\nwget http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.32.zip\nunzip Trimmomatic-0.32.zip\ncd Trimmomatic-0.32\nchmod +x trimmomatic-0.32.jar    Install SRAtoolkit   cd $HOME\nwget http://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/2.4.2/sratoolkit.2.4.2-ubuntu64.tar.gz\ntar -zxf sratoolkit.2.4.2-ubuntu64.tar.gz\nPATH=$PATH:/home/ubuntu/sratoolkit.2.4.2-ubuntu64/bin    Install SPAdes   wget http://spades.bioinf.spbau.ru/release3.1.1/SPAdes-3.1.1-Linux.tar.gz\ntar -zxf SPAdes-3.1.1-Linux.tar.gz\ncd SPAdes-3.1.1-Linux\nPATH=$PATH:$(pwd)/bin    Download 3.5kb MP library   cd /mnt\nwget ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/ERR/ERR022/ERR022558/ERR022558.sra   Download 10kb MP library   cd /mnt\nwget ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/ERR/ERR022/ERR022557/ERR022557.sra   Download PE library #1   cd /mnt\nwget ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/ERR/ERR019/ERR019273/ERR019273.sra    Download PE library #2   cd /mnt\nwget ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/ERR/ERR019/ERR019275/ERR019275.sra    Extract fastQ from sra format.   cd /mnt\n\n#this is a basic for loop. Copy is all as 1 line.\n\nfor i in `ls *sra`; do\n    fastq-dump --split-files --split-spot $i;\n    rm $i;\ndone    Error Correct Data   mkdir /mnt/ec\ncd /mnt/ec\nlighter -r /mnt/ERR019273_1.fastq -r /mnt/ERR019273_2.fastq -t 32 -k 21 45000000 .1\nlighter -r /mnt/ERR022557_1.fastq -r /mnt/ERR022557_2.fastq -t 32 -k 21 45000000 .1\nlighter -r /mnt/ERR022558_1.fastq -r /mnt/ERR022558_2.fastq -t 32 -k 21 45000000 .1\nlighter -r /mnt/ERR019275_1.fastq -r /mnt/ERR019275_2.fastq -t 32 -k 21 45000000 .1\n\n#remove the raw files.\n\nrm *fastq     trim the data:   mkdir /mnt/trim\ncd /mnt/trim\n#paste the below lines together as 1 command\n\njava -Xmx10g -jar $HOME/Trimmomatic-0.32/trimmomatic-0.32.jar PE \\\n-threads 32 -baseout PE_lib1.fq \\\n/mnt/ec/ERR019273_1.cor.fq \\\n/mnt/ec/ERR019273_2.cor.fq \\\nILLUMINACLIP:$HOME/Trimmomatic-0.32/adapters/TruSeq2-PE.fa:2:30:10 \\\nSLIDINGWINDOW:4:2 \\\nLEADING:2 \\\nTRAILING:2 \\\nMINLEN:25\n\n#paste the below lines together as 1 command\n\njava -Xmx10g -jar $HOME/Trimmomatic-0.32/trimmomatic-0.32.jar PE \\\n-threads 32 -baseout PE_lib2.fq \\\n/mnt/ec/ERR019275_1.cor.fq \\\n/mnt/ec/ERR019275_2.cor.fq \\\nILLUMINACLIP:$HOME/Trimmomatic-0.32/adapters/TruSeq2-PE.fa:2:30:10 \\\nSLIDINGWINDOW:4:2 \\\nLEADING:2 \\\nTRAILING:2 \\\nMINLEN:25\n\n#paste the below lines together as 1 command\n\njava -Xmx10g -jar $HOME/Trimmomatic-0.32/trimmomatic-0.32.jar PE \\\n-threads 32 -baseout MP10000.fq \\\n/mnt/ec/ERR022557_1.cor.fq \\\n/mnt/ec/ERR022557_2.cor.fq \\\nILLUMINACLIP:$HOME/Trimmomatic-0.32/adapters/TruSeq2-PE.fa:2:30:10 \\\nSLIDINGWINDOW:4:2 \\\nLEADING:2 \\\nTRAILING:2 \\\nMINLEN:25\n\n#paste the below lines together as 1 command\n\njava -Xmx10g -jar $HOME/Trimmomatic-0.32/trimmomatic-0.32.jar PE \\\n-threads 32 -baseout MP3500.fq \\\n/mnt/ec/ERR022558_1.cor.fq \\\n/mnt/ec/ERR022558_2.cor.fq \\\nILLUMINACLIP:$HOME/Trimmomatic-0.32/adapters/TruSeq2-PE.fa:2:30:10 \\\nSLIDINGWINDOW:4:2 \\\nLEADING:2 \\\nTRAILING:2 \\\nMINLEN:25   mkdir /mnt/\n\n#remove the corrected files.\n\nrm *fq   Assembly. This you may want to do next week. Alternatively, you can put it in a tmux window and let it run. You'd have to login later however to download the assembled genome.     mkdir spades\ncd spades\n\nspades.py -t 32 -m 60 \\\n--pe1-1 /mnt/trim/PE_lib1_1P.fq \\\n--pe1-2 /mnt/trim/PE_lib1_2P.fq \\\n--pe2-1 /mnt/trim/PE_lib2_1P.fq \\\n--pe2-2 /mnt/trim/PE_lib2_2P.fq \\\n--mp1-1 /mnt/trim/MP3500_1P.fq \\\n--mp1-2 /mnt/trim/MP3500_2P.fq \\\n--mp2-1 /mnt/trim/MP10000_1P.fq \\\n--mp2-2 /mnt/trim/MP10000_2P.fq \\\n-o Pfal --only-assembler", 
            "title": "Lab 9: Genome assembly"
        }, 
        {
            "location": "/lab_lessons/README/", 
            "text": "This folder contains the (version 1) lab lessons for Gen711/811, released under a CC-BY license. \n\n\nThis class was taught for the 1st time in Fall 2014 at the University of New Hampshire to a class of 25 (half undergrad, halF grad) with NO programming experience. We spent 2 hours per week in the computer lab doing these labs. The course website is here: http://genomebio.org/Gen711/\n\n\nPlease feel free to fork/send me pull requests, or otherwise incorporate as you see fit.", 
            "title": "README"
        }, 
        {
            "location": "/lab_lessons/alignment/", 
            "text": "Lab 3: Alignment\n\n\nDuring this lab, we will acquaint ourselves with alignment. Your objectives are:\n\n\n\n\n\n\nFamiliarize yourself with the software, how to execute it, how to visualize results.\n\n\n\n\n\n\nRegarding your dataset, tell me how some of these genes are related to their homologous copies.\n\n\n\n\n\n\nStep 1: Launch and AMI. For this exercise, we will use a c4.xlarge instance. This instance has 4 cores.\n\n\n\nssh -i ~/Downloads/your.pem ubuntu@???-???-???-???\n\n\n\n\n\nUpdate the software\n\n\nsudo apt-get update \n sudo apt-get -y upgrade\n\n\n\n\nOK, what are these commands?  \nsudo\n is the command that tells the computer that we have admin privileges. Try running the commands without the sudo -- it will complain that you don't have admin privileges or something like that. \nCareful here, using sudo means that you can do something really bad to your own computer -- like delete everything\n, so use with caution. It's not a big worry when using AWS, as this is a virtual machine- fixing your worst mistake is as easy as just terminating the instance and restarting.\n\n\nSo now that we have updates the software, lets see how to add new software. Same basic command, but instead of the \nupdate\n or \nupgrade\n command, we're using \ninstall\n. EASY!!\n\n\nsudo apt-get -y install build-essential git\n\n\n\n\nInstall LinuxBrew.\n\n\nhttp://linuxbrew.sh/. LinuxBrew can be installed in your home directory and does not require root access. The same package manager can be used on both your Linux server and your Mac laptop.\n\n\nInstall Ruby\n\n\nLinuxBrew needs Ruby to install\n\n\ncd\nwget https://keybase.io/mpapis/key.asc\ngpg --import key.asc\n\\curl -sSL https://get.rvm.io | bash -s stable --ruby\nsource /home/ubuntu/.rvm/scripts/rvm\n\n\n\n\nInstall LinuxBrew itself\n\n\nsudo mkdir /home/linuxbrew\nsudo chown $USER:$USER /home/linuxbrew\ngit clone https://github.com/Linuxbrew/brew.git /home/linuxbrew/.linuxbrew\necho 'export PATH=\n/home/linuxbrew/.linuxbrew/bin:$PATH\n' \n ~/.profile\necho 'export MANPATH=\n/home/linuxbrew/.linuxbrew/share/man:$MANPATH\n' \n ~/.profile\necho 'export INFOPATH=\n/home/linuxbrew/.linuxbrew/share/info:$INFOPATH\n' \n ~/.profile\nsource ~/.profile\nbrew tap homebrew/science\nbrew update\nbrew doctor\n\n\n\n\nInstall mafft and RAxML and BLAST\n\n\nYes, installing software is easy.. \nbrew install XXX\n. Do you have a favorite software package. Try and install it, in addition to the other things we are installing.\n\n\nbrew install mafft raxml blast aria2\n\n\n\n\nInstall BioPython\n\n\nBioPython is a python package that does a lot of common tasks related to sequence manipulation.\n\n\npip install biopython\n\n\n\n\nDownload the blast database and unzip it\n\n\nWe are downloading SwissProt, which has about 550,000 curated protein sequences.\n\n\nNote we are using a program called \naria2c\n to download. This is a something that, for huuuuge files, can significantly speed up the process. For smaller things, I'll still use \ncurl\n\n\naria2c ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz\ngzip -d uniprot_sprot.fasta.gz\n\n\n\n\nDownload the query sequences\n\n\naria2c https://s3.amazonaws.com/gen711/dataset1.pep\n\n\n\n\nMake blast database and blast\n\n\nmakeblastdb -in uniprot_sprot.fasta -out uniprot -dbtype prot\n\n\n\n\nFor the query sequences\n, I am going to pull out 2 specific sequences from the larger dataset \ndataset1.pep\n. These are both HOX genes that I have identified before class. The command \ngrep\n finds words/numbers/symbols in files. The \n-A1\n flag that I pass to the \ngrep\n command asks that grep returns the line that matches, but also the line just below the match. We're downloading a fasta record.. think about why this makes sense.\n\n\nRemember that the \n sends the results of the command to a file, in this case names \nquery.pep\n.\n\n\ngrep -A1 'ENSPTRP00000032491\\|ENSPTRP00000032494' dataset1.pep \n query.pep\n\n\n\n\nNow that I have a couple of query sequences, we are ready to blast.\n\n\nblastp -evalue 8e-8 -num_threads 4 -db uniprot -query query.pep -max_target_seqs 3 -outfmt \n6 qseqid score pident evalue stitle\n\n\n\n\n\nYou will see the results in a table with 4 columns. Use \nblastp -help\n to see what the results mean. Test out some of the blast options. Try changing the matrix size using \n-matrix\n, to see how these changes affect the results.\n\n\nMake a file that contains the sequences for the blast hits you just discovered\n\n\n### You need a little script to help you pull out the sequences\n\ncurl -LO https://raw.githubusercontent.com/macmanes-lab/general/master/filter.py\n\n### Now open a file, and make a list of the names of the proteins you just identified using blast\n\nnano cool_prots.list\n\n### paste this list, and save and quit nano\n\nHXA2_HUMAN\nHXA2_BOVIN\nHXA2_PAPAN\nHXA3_HUMAN\nHXA3_MOUSE\nHXA3_BOVIN\nHXA9_HUMAN\n\n\n### Now sue the script to pull out the proteins into a file. Note that this script does something pretty fancy. It uses process substitution (https://en.wikipedia.org/wiki/Process_substitution). To understand process substitution you also need to get familiar with the ideas of stdout and stdin (https://en.wikipedia.org/wiki/Standard_streams)\n\npython filter.py uniprot_sprot.fasta \\\n\n(grep -f cool_prots.list uniprot_sprot.fasta | awk '{print $1}' | sed 's_\n__') \n cool_prots.fasta\n\n\n\n\n\nNow, make a file that contains BOTH  the query sequences AND the sequences we found by blasting.\n\n\nWhat does \ncat\n do??\n\n\n\ncat query.pep cool_prots.fasta \n for_alignment.pep\n\n\n\n\n\nUse mafft for alignment\n\n\nWhat are the different options to \nmafft\n\n\n\nmafft --reorder --bl 80 --localpair --thread 4 for_alignment.pep \n for.tree\n\n\n\n\n\nRAxML\n\n\nWhat are the different options to \nRaxML\n\n\nMake a phylogeny\n\n\n\nraxmlHPC-PTHREADS -f a -m PROTCATBLOSUM62 -T 4 -x 34 -N 100 -n tree -s for.tree -p 35\n\n\n\n\n\n\nCopy phylogeny and view online.\n\n\n\nless RAxML_bipartitionsBranchLabels.tree\n\n    #copy this info.\n\n\n\n\n\nVisualize tree on website: http://www.evolgenius.info/evolview/\n\n\n\n\nClick on \"Use without an account\"\n\n\nClick on the folder icon in the top-left part of the page.\n\n\nPaste in the code from your terminal. \nFYI, this is the NEWICK tree format\n, yes, named after Newick's Restaurant just down the road from us!!\n\n\nFind the HOX9 gene - this is the outgroup sequence we will use to rood the tree. Hover over the branch and it will turn red - a box will open, click \"reroot here\"\n\n\n\n\nTERMINATE YOUR INSTANCE", 
            "title": "Alignment"
        }, 
        {
            "location": "/lab_lessons/alignment/#lab-3-alignment", 
            "text": "During this lab, we will acquaint ourselves with alignment. Your objectives are:    Familiarize yourself with the software, how to execute it, how to visualize results.    Regarding your dataset, tell me how some of these genes are related to their homologous copies.    Step 1: Launch and AMI. For this exercise, we will use a c4.xlarge instance. This instance has 4 cores.  \nssh -i ~/Downloads/your.pem ubuntu@???-???-???-???  Update the software  sudo apt-get update   sudo apt-get -y upgrade  OK, what are these commands?   sudo  is the command that tells the computer that we have admin privileges. Try running the commands without the sudo -- it will complain that you don't have admin privileges or something like that.  Careful here, using sudo means that you can do something really bad to your own computer -- like delete everything , so use with caution. It's not a big worry when using AWS, as this is a virtual machine- fixing your worst mistake is as easy as just terminating the instance and restarting.  So now that we have updates the software, lets see how to add new software. Same basic command, but instead of the  update  or  upgrade  command, we're using  install . EASY!!  sudo apt-get -y install build-essential git", 
            "title": "Lab 3: Alignment"
        }, 
        {
            "location": "/lab_lessons/alignment/#install-linuxbrew", 
            "text": "http://linuxbrew.sh/. LinuxBrew can be installed in your home directory and does not require root access. The same package manager can be used on both your Linux server and your Mac laptop.", 
            "title": "Install LinuxBrew."
        }, 
        {
            "location": "/lab_lessons/alignment/#install-ruby", 
            "text": "LinuxBrew needs Ruby to install  cd\nwget https://keybase.io/mpapis/key.asc\ngpg --import key.asc\n\\curl -sSL https://get.rvm.io | bash -s stable --ruby\nsource /home/ubuntu/.rvm/scripts/rvm", 
            "title": "Install Ruby"
        }, 
        {
            "location": "/lab_lessons/alignment/#install-linuxbrew-itself", 
            "text": "sudo mkdir /home/linuxbrew\nsudo chown $USER:$USER /home/linuxbrew\ngit clone https://github.com/Linuxbrew/brew.git /home/linuxbrew/.linuxbrew\necho 'export PATH= /home/linuxbrew/.linuxbrew/bin:$PATH '   ~/.profile\necho 'export MANPATH= /home/linuxbrew/.linuxbrew/share/man:$MANPATH '   ~/.profile\necho 'export INFOPATH= /home/linuxbrew/.linuxbrew/share/info:$INFOPATH '   ~/.profile\nsource ~/.profile\nbrew tap homebrew/science\nbrew update\nbrew doctor", 
            "title": "Install LinuxBrew itself"
        }, 
        {
            "location": "/lab_lessons/alignment/#install-mafft-and-raxml-and-blast", 
            "text": "Yes, installing software is easy..  brew install XXX . Do you have a favorite software package. Try and install it, in addition to the other things we are installing.  brew install mafft raxml blast aria2", 
            "title": "Install mafft and RAxML and BLAST"
        }, 
        {
            "location": "/lab_lessons/alignment/#install-biopython", 
            "text": "BioPython is a python package that does a lot of common tasks related to sequence manipulation.  pip install biopython", 
            "title": "Install BioPython"
        }, 
        {
            "location": "/lab_lessons/alignment/#download-the-blast-database-and-unzip-it", 
            "text": "We are downloading SwissProt, which has about 550,000 curated protein sequences.  Note we are using a program called  aria2c  to download. This is a something that, for huuuuge files, can significantly speed up the process. For smaller things, I'll still use  curl  aria2c ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz\ngzip -d uniprot_sprot.fasta.gz", 
            "title": "Download the blast database and unzip it"
        }, 
        {
            "location": "/lab_lessons/alignment/#download-the-query-sequences", 
            "text": "aria2c https://s3.amazonaws.com/gen711/dataset1.pep", 
            "title": "Download the query sequences"
        }, 
        {
            "location": "/lab_lessons/alignment/#make-blast-database-and-blast", 
            "text": "makeblastdb -in uniprot_sprot.fasta -out uniprot -dbtype prot  For the query sequences , I am going to pull out 2 specific sequences from the larger dataset  dataset1.pep . These are both HOX genes that I have identified before class. The command  grep  finds words/numbers/symbols in files. The  -A1  flag that I pass to the  grep  command asks that grep returns the line that matches, but also the line just below the match. We're downloading a fasta record.. think about why this makes sense.  Remember that the   sends the results of the command to a file, in this case names  query.pep .  grep -A1 'ENSPTRP00000032491\\|ENSPTRP00000032494' dataset1.pep   query.pep  Now that I have a couple of query sequences, we are ready to blast.  blastp -evalue 8e-8 -num_threads 4 -db uniprot -query query.pep -max_target_seqs 3 -outfmt  6 qseqid score pident evalue stitle   You will see the results in a table with 4 columns. Use  blastp -help  to see what the results mean. Test out some of the blast options. Try changing the matrix size using  -matrix , to see how these changes affect the results.", 
            "title": "Make blast database and blast"
        }, 
        {
            "location": "/lab_lessons/alignment/#make-a-file-that-contains-the-sequences-for-the-blast-hits-you-just-discovered", 
            "text": "### You need a little script to help you pull out the sequences\n\ncurl -LO https://raw.githubusercontent.com/macmanes-lab/general/master/filter.py\n\n### Now open a file, and make a list of the names of the proteins you just identified using blast\n\nnano cool_prots.list\n\n### paste this list, and save and quit nano\n\nHXA2_HUMAN\nHXA2_BOVIN\nHXA2_PAPAN\nHXA3_HUMAN\nHXA3_MOUSE\nHXA3_BOVIN\nHXA9_HUMAN\n\n\n### Now sue the script to pull out the proteins into a file. Note that this script does something pretty fancy. It uses process substitution (https://en.wikipedia.org/wiki/Process_substitution). To understand process substitution you also need to get familiar with the ideas of stdout and stdin (https://en.wikipedia.org/wiki/Standard_streams)\n\npython filter.py uniprot_sprot.fasta \\ (grep -f cool_prots.list uniprot_sprot.fasta | awk '{print $1}' | sed 's_ __')   cool_prots.fasta", 
            "title": "Make a file that contains the sequences for the blast hits you just discovered"
        }, 
        {
            "location": "/lab_lessons/alignment/#now-make-a-file-that-contains-both-the-query-sequences-and-the-sequences-we-found-by-blasting", 
            "text": "What does  cat  do??  \ncat query.pep cool_prots.fasta   for_alignment.pep", 
            "title": "Now, make a file that contains BOTH  the query sequences AND the sequences we found by blasting."
        }, 
        {
            "location": "/lab_lessons/alignment/#use-mafft-for-alignment", 
            "text": "What are the different options to  mafft  \nmafft --reorder --bl 80 --localpair --thread 4 for_alignment.pep   for.tree", 
            "title": "Use mafft for alignment"
        }, 
        {
            "location": "/lab_lessons/alignment/#raxml", 
            "text": "What are the different options to  RaxML  Make a phylogeny  \nraxmlHPC-PTHREADS -f a -m PROTCATBLOSUM62 -T 4 -x 34 -N 100 -n tree -s for.tree -p 35", 
            "title": "RAxML"
        }, 
        {
            "location": "/lab_lessons/alignment/#copy-phylogeny-and-view-online", 
            "text": "less RAxML_bipartitionsBranchLabels.tree\n\n    #copy this info.  Visualize tree on website: http://www.evolgenius.info/evolview/   Click on \"Use without an account\"  Click on the folder icon in the top-left part of the page.  Paste in the code from your terminal.  FYI, this is the NEWICK tree format , yes, named after Newick's Restaurant just down the road from us!!  Find the HOX9 gene - this is the outgroup sequence we will use to rood the tree. Hover over the branch and it will turn red - a box will open, click \"reroot here\"", 
            "title": "Copy phylogeny and view online."
        }, 
        {
            "location": "/lab_lessons/alignment/#terminate-your-instance", 
            "text": "", 
            "title": "TERMINATE YOUR INSTANCE"
        }, 
        {
            "location": "/lab_lessons/mock.exam1/", 
            "text": "MOCK EXAM 1\n\n\nTo help you prepare for the exam practical, let's do a mock exam. These are the types of questions I will ask you during the exam. I will give you points for each section, with the idea that I have to help you get past a particular section, you lose those points, but if you can do the rest of the steps, you can still get credit for that.\n\n\n\n\n\n\nLaunch a \nc4.2xlarge\n instance.  \n_\n 5 points\n\n\n\n\n\n\nUpdate your machine and install the software using \napt-get\n just like we did in lab 2,3,4,5. \n_\n__ 5 points\n\n\n\n\n\n\nInstall \nLinuxBrew\n and \nskewer\n and \njellyfish\n like lab 4. \n_\n_\n_\n 10 points.\n\n\n\n\n\n\nDownload the datasets located here: \nhttps://s3.amazonaws.com/gen711/TruSeq3-PE.fa\n, \nhttps://s3.amazonaws.com/Mc_Transcriptome/Thomas_McBr1_R1.PF.fastq.gz\n and \nhttps://s3.amazonaws.com/Mc_Transcriptome/Thomas_McBr1_R2.PF.fastq.gz\n.  \n_\n_\n___ 5 points\n\n\n\n\n\n\nTrim the dataset like we did in Lab 4 using a Phred score of 28.  \n_\n_\n_\n_ 10 points\n\n\n\n\n\n\nCount the remaining reads using the command \nzgrep -c @HWI skewer-trimmed-pair1.fastq\n and put the number you get here \n_\n_\n_ 15 points", 
            "title": "Mock.exam1"
        }, 
        {
            "location": "/lab_lessons/mock.exam1/#mock-exam-1", 
            "text": "To help you prepare for the exam practical, let's do a mock exam. These are the types of questions I will ask you during the exam. I will give you points for each section, with the idea that I have to help you get past a particular section, you lose those points, but if you can do the rest of the steps, you can still get credit for that.    Launch a  c4.2xlarge  instance.   _  5 points    Update your machine and install the software using  apt-get  just like we did in lab 2,3,4,5.  _ __ 5 points    Install  LinuxBrew  and  skewer  and  jellyfish  like lab 4.  _ _ _  10 points.    Download the datasets located here:  https://s3.amazonaws.com/gen711/TruSeq3-PE.fa ,  https://s3.amazonaws.com/Mc_Transcriptome/Thomas_McBr1_R1.PF.fastq.gz  and  https://s3.amazonaws.com/Mc_Transcriptome/Thomas_McBr1_R2.PF.fastq.gz .   _ _ ___ 5 points    Trim the dataset like we did in Lab 4 using a Phred score of 28.   _ _ _ _ 10 points    Count the remaining reads using the command  zgrep -c @HWI skewer-trimmed-pair1.fastq  and put the number you get here  _ _ _ 15 points", 
            "title": "MOCK EXAM 1"
        }, 
        {
            "location": "/lab_lessons/trimming/", 
            "text": "Lab4: Trimming fastQ\n\n\nStep 1: Launch the AMI. For this exercise, we will use a c4.2xlarge instance. \nIMPORTANT DETAIL!!!!!!!!!\n We need to have more hard drive space for this exercise. So when you select the machine type, don't click \nReview and Launch\n like you normally do. Instead, go to near the top of teh page and click \n4. Add Storage\n. You'll see a column labelled \nSize (GiB)\n with a 8 under that.. Change that 8 to 100. Now click \nReview and Launch\n like normal. You now have a computer with a hard drive of size 100Gb.\n\n\nssh -i ~/Downloads/?????.pem ubuntu@ec2-???-???-???-???\n\n\n\n\nUpdate Software\n\n\nsudo apt-get update \n sudo apt-get -y upgrade\n\n\n\n\nInstall other software\n\n\nsudo apt-get -y install build-essential git\n\n\n\n\nInstall LinuxBrew\n\n\n\ncd\nwget https://keybase.io/mpapis/key.asc\ngpg --import key.asc\n\\curl -sSL https://get.rvm.io | bash -s stable --ruby\nsource /home/ubuntu/.rvm/scripts/rvm\n\n\nsudo mkdir /home/linuxbrew\nsudo chown $USER:$USER /home/linuxbrew\ngit clone https://github.com/Linuxbrew/brew.git /home/linuxbrew/.linuxbrew\necho 'export PATH=\n/home/linuxbrew/.linuxbrew/bin:$PATH\n' \n ~/.profile\necho 'export MANPATH=\n/home/linuxbrew/.linuxbrew/share/man:$MANPATH\n' \n ~/.profile\necho 'export INFOPATH=\n/home/linuxbrew/.linuxbrew/share/info:$INFOPATH\n' \n ~/.profile\nsource ~/.profile\nbrew tap homebrew/science\nbrew update\nbrew doctor\n\n\n\n\n\n\nDownload data, and uncompress them.. Let's put this in a tmux window so we can get to doing other things.. remember you need paste the tmux relevant commands one at a time.\n\n\n\ntmux new -s download\n\ncd $HOME \n mkdir reads \n cd reads\ncurl -LO https://s3.amazonaws.com/gen711/TruSeq3-PE.fa\ncurl -L https://s3.amazonaws.com/Mc_Transcriptome/Thomas_McBr1_R1.PF.fastq.gz \n kidney.1.fq.gz \n\ncurl -L https://s3.amazonaws.com/Mc_Transcriptome/Thomas_McBr1_R2.PF.fastq.gz \n kidney.2.fq.gz\n\n#then this to get out of tmux\n\nctl-b d\n\n\n\n\n\nInstall Skewer, a trimming tool. Seqtk, a sequence manipulation tool, and Python..\n\n\n\nbrew install skewer seqtk python jellyfish\n\n\n\n\n\nnow go back to the download tmux window to see if//wait for the download\n\n\ntmux at -t download\n\n#IMPORTANT - when it is done downloading, kill the window\n\nctl-d\n\n\n\n\n\nRun khmer. Make sure to look at the manual.\n\n\n\ntmux new -s kmer\nmkdir $HOME/kmer_analysis \n cd $HOME/kmer_analysis\n\n#IMPORTANT DETAIL: usually pasting things in 1 at a time is fine - except here... When you see ``\\`` at the end of lines, this means copy the 2 (or 3 or 4) lines together.\n\n\n#do trimming at P2\n\ntrim=2\nseqtk mergepe ~/reads/kidney.1.fq.gz ~/reads/kidney.2.fq.gz \\\n| skewer --stdout -l 25 -m pe --mean-quality $trim --end-quality $trim -t 8 -x $HOME/reads/TruSeq3-PE.fa - \\\n| jellyfish count -s 1000000 -m 25 -t 8 -o $trim.counts.jf -C /dev/stdin\n\njellyfish histo $trim.counts.jf \n $trim.counts.histo\n\n#do trimming at P30\n\ntrim=30\nseqtk mergepe ~/reads/kidney.1.fq.gz ~/reads/kidney.2.fq.gz \\\n| skewer --stdout -l 25 -m pe --mean-quality $trim --end-quality $trim -t 8 -x $HOME/reads/TruSeq3-PE.fa - \\\n| jellyfish count -s 1000000 -m 25 -t 8 -o $trim.counts.jf -C /dev/stdin\n\njellyfish histo $trim.counts.jf \n $trim.counts.histo\n\n\n # to exit out of the tmux window, if you want to.\n\n ctl-b d\n\n\n\n\n\nWait for these things to be done.. Use \ntop -c\n to do this.. Remember \nq\n gets you outta \ntop\n.\n\n\nOpen up a new terminal (tab) or window using the buttons command-t. You're going to download the files you created on teh AWS machine to the MAC your using in the lab.\n\n\nscp -i ~/Downloads/????.pem ubuntu@ec2-??-???-???-??:~/kmer_analysis/*histo ~/Downloads/\n\n\n\n\n\n\nNow look at the and \n.histo\n file.  which is the plot of quality containing both the mean quality as well as that for each tile. I want you to plot the distribution using R and RStudio.\n\n\nOPEN RStudio - this should be installed on your Mac. These commands you'll type into RStudio, NOT the terminal.\n\n\n\n#Import Data\np2 \n- read.table(\n~/Downloads/2.counts.histo\n, quote=\n\\\n, comment.char=\n)\np30 \n- read.table(\n~/Downloads/30.counts.histo\n, quote=\n\\\n, comment.char=\n)\n\npar(mfcol=c(2,1))\n\nplot(p2$V2[2:10] ~ 1, type='p', lwd=5,\n    col='blue', frame.plot=F, xlab='25-mer frequency', ylab='kmer count',\n    main='Kmer distribution in sample with different trimming thresholds')\n\nlines(p30$V2[2:10] ~ 1, type='p', lwd=5,\n    col='red')\n\nplot(p2$V2[2:30] - p30$V2[2:30], type='p',\n    xlim=c(2,20), xaxs=\ni\n, yaxs=\ni\n, frame.plot=F,\n    ylim=c(0,1500000), col='red', xlab='kmer frequency',\n    lwd=4, ylab='count',\n    main='Diff in 25mer counts of freq 2 to 20 \\n Phred2 vs. Phred30')\n\n\n\n\nTerminate your instance!!!", 
            "title": "Trimming"
        }, 
        {
            "location": "/lab_lessons/trimming/#lab4-trimming-fastq", 
            "text": "Step 1: Launch the AMI. For this exercise, we will use a c4.2xlarge instance.  IMPORTANT DETAIL!!!!!!!!!  We need to have more hard drive space for this exercise. So when you select the machine type, don't click  Review and Launch  like you normally do. Instead, go to near the top of teh page and click  4. Add Storage . You'll see a column labelled  Size (GiB)  with a 8 under that.. Change that 8 to 100. Now click  Review and Launch  like normal. You now have a computer with a hard drive of size 100Gb.  ssh -i ~/Downloads/?????.pem ubuntu@ec2-???-???-???-???  Update Software  sudo apt-get update   sudo apt-get -y upgrade  Install other software  sudo apt-get -y install build-essential git  Install LinuxBrew  \ncd\nwget https://keybase.io/mpapis/key.asc\ngpg --import key.asc\n\\curl -sSL https://get.rvm.io | bash -s stable --ruby\nsource /home/ubuntu/.rvm/scripts/rvm\n\n\nsudo mkdir /home/linuxbrew\nsudo chown $USER:$USER /home/linuxbrew\ngit clone https://github.com/Linuxbrew/brew.git /home/linuxbrew/.linuxbrew\necho 'export PATH= /home/linuxbrew/.linuxbrew/bin:$PATH '   ~/.profile\necho 'export MANPATH= /home/linuxbrew/.linuxbrew/share/man:$MANPATH '   ~/.profile\necho 'export INFOPATH= /home/linuxbrew/.linuxbrew/share/info:$INFOPATH '   ~/.profile\nsource ~/.profile\nbrew tap homebrew/science\nbrew update\nbrew doctor  Download data, and uncompress them.. Let's put this in a tmux window so we can get to doing other things.. remember you need paste the tmux relevant commands one at a time.  \ntmux new -s download\n\ncd $HOME   mkdir reads   cd reads\ncurl -LO https://s3.amazonaws.com/gen711/TruSeq3-PE.fa\ncurl -L https://s3.amazonaws.com/Mc_Transcriptome/Thomas_McBr1_R1.PF.fastq.gz   kidney.1.fq.gz  \ncurl -L https://s3.amazonaws.com/Mc_Transcriptome/Thomas_McBr1_R2.PF.fastq.gz   kidney.2.fq.gz\n\n#then this to get out of tmux\n\nctl-b d  Install Skewer, a trimming tool. Seqtk, a sequence manipulation tool, and Python..  \nbrew install skewer seqtk python jellyfish  now go back to the download tmux window to see if//wait for the download  tmux at -t download\n\n#IMPORTANT - when it is done downloading, kill the window\n\nctl-d  Run khmer. Make sure to look at the manual.  \ntmux new -s kmer\nmkdir $HOME/kmer_analysis   cd $HOME/kmer_analysis\n\n#IMPORTANT DETAIL: usually pasting things in 1 at a time is fine - except here... When you see ``\\`` at the end of lines, this means copy the 2 (or 3 or 4) lines together.\n\n\n#do trimming at P2\n\ntrim=2\nseqtk mergepe ~/reads/kidney.1.fq.gz ~/reads/kidney.2.fq.gz \\\n| skewer --stdout -l 25 -m pe --mean-quality $trim --end-quality $trim -t 8 -x $HOME/reads/TruSeq3-PE.fa - \\\n| jellyfish count -s 1000000 -m 25 -t 8 -o $trim.counts.jf -C /dev/stdin\n\njellyfish histo $trim.counts.jf   $trim.counts.histo\n\n#do trimming at P30\n\ntrim=30\nseqtk mergepe ~/reads/kidney.1.fq.gz ~/reads/kidney.2.fq.gz \\\n| skewer --stdout -l 25 -m pe --mean-quality $trim --end-quality $trim -t 8 -x $HOME/reads/TruSeq3-PE.fa - \\\n| jellyfish count -s 1000000 -m 25 -t 8 -o $trim.counts.jf -C /dev/stdin\n\njellyfish histo $trim.counts.jf   $trim.counts.histo\n\n\n # to exit out of the tmux window, if you want to.\n\n ctl-b d  Wait for these things to be done.. Use  top -c  to do this.. Remember  q  gets you outta  top .  Open up a new terminal (tab) or window using the buttons command-t. You're going to download the files you created on teh AWS machine to the MAC your using in the lab.  scp -i ~/Downloads/????.pem ubuntu@ec2-??-???-???-??:~/kmer_analysis/*histo ~/Downloads/  Now look at the and  .histo  file.  which is the plot of quality containing both the mean quality as well as that for each tile. I want you to plot the distribution using R and RStudio.  OPEN RStudio - this should be installed on your Mac. These commands you'll type into RStudio, NOT the terminal.  \n#Import Data\np2  - read.table( ~/Downloads/2.counts.histo , quote= \\ , comment.char= )\np30  - read.table( ~/Downloads/30.counts.histo , quote= \\ , comment.char= )\n\npar(mfcol=c(2,1))\n\nplot(p2$V2[2:10] ~ 1, type='p', lwd=5,\n    col='blue', frame.plot=F, xlab='25-mer frequency', ylab='kmer count',\n    main='Kmer distribution in sample with different trimming thresholds')\n\nlines(p30$V2[2:10] ~ 1, type='p', lwd=5,\n    col='red')\n\nplot(p2$V2[2:30] - p30$V2[2:30], type='p',\n    xlim=c(2,20), xaxs= i , yaxs= i , frame.plot=F,\n    ylim=c(0,1500000), col='red', xlab='kmer frequency',\n    lwd=4, ylab='count',\n    main='Diff in 25mer counts of freq 2 to 20 \\n Phred2 vs. Phred30')", 
            "title": "Lab4: Trimming fastQ"
        }, 
        {
            "location": "/lab_lessons/trimming/#terminate-your-instance", 
            "text": "", 
            "title": "Terminate your instance!!!"
        }, 
        {
            "location": "/lab_lessons/unix/", 
            "text": "Lab 1\n\n\nDuring this lab, we will acquaint ourselves with the Unix terminal, learn how to access data, install software, and\u00a0 find things. \nit is absolutely critical that you master these skills\n, so please ask questions if confused.\n\n\nStep 1: Launch and AMI. For this exercise, a t1.micro will be sufficient.\n\n\nssh -i ~/Downloads/your.pem ubuntu@ec2-???-???-???-???.compute-1.amazonaws.com\n\n\n\nThe machine you are using is Linux Ubuntu: Ubuntu is an operating system you can use (I do) on your laptop or desktop. One of the nice things about this OS is the ability to update the software, easily.\u00a0 The command \nsudo apt-get update\n checks a server for updates to existing software.\n\n\nsudo apt-get update\n\n\n\nThe upgrade command actually installs any of the required updates.\n\n\nsudo apt-get upgrade\n\n\n\nOK, what are these commands?\u00a0 \nsudo\n is the command that tells the computer that we have admin privileges. Try running the commands without the sudo -- it will complain that you don't have admin privileges or something like that. \nCareful here, using sudo means that you can do something really bad to your own computer -- like delete everything\n, so use with caution. It's not a big worry when using AWS, as this is a virtual machine- fixing your worst mistake is as easy as just terminating the instance and restarting.\n\n\nSo now that we have updates the software, lets see how to add new software. Same basic command, but instead of the \nupdate\n or \nupgrade\n command, we're using \ninstall\n. EASY!!\n\n\nsudo apt-get -y install tmux git curl gcc make g++ python-dev unzip \\\n    default-jre\n\n\n\nAfter you run this command, try something else - try to install something else. R (a stats package - more on this wonderful software later). The package is named \nr-base-core\n. See if you can install it!! Installing software on Linux is easy (so long as there is a downloadable package - more on when no such package exists later in lab)\n\n\nBTW, did you notice the \n\\\n at the end of line 1 in the above code snippett?? That is a special character we use to break up a single line of code over 2 or more lines. You'll see me use this a lot!\n\n\nOK, lets try our hands at navigating around on the command line - it is not scary!\n\n\nImportant UNIX rules\n\n\n\n\nEverything is case sensitive. Gen711 is not the same as gen711\n\n\nSpaces in file names should be avoided\n\n\nThe unix $PATH is the collection of locations where the computer looks for executables (programs)\n\n\n\n\nFolders and Files are all you have. If you want to access one of these, you need to tell the computer \nEXACTLY\n where it is. \n/home/macmanes/gen711/exam1_key.txt\n will work (assuming you've spelled things correctly, and that the file really exists in that location), but \nexam1_key.txt\n may not.\n\n\n\n\n\n\nLines that begin with a \n#\n are comments.\n\n\n\n\n\n\nBasic shell commands\n\n\nthe \npwd\n command returns your current location.\n\n\npwd\n\n\n\nthe \nls\n command lists the files and folders present in your current directory.\u00a0 Try \nls -lt\n and \nls -lth\n. \nWhat is the difference between these commands?\n Try typing \nman ls\n to learn about all the different flags.\n\n\nls -l\n\n\n\ncreate a file\n\n\nnano hello.txt\n#The nano text editor will appear -\n type something\nThis is my 1st unix file\nCTL-x\ny\n#typing n would get rid of the text you just wrote.\n\n\n\nlook at the file, there are several ways to look at the file\n\n\nhead -5 hello.txt #this shows you the 1st 5 lines of the file\nmore hello.txt #this shows you the whole file, 1 screen at a time. Space bar to advance, q to quit\n\n\n\nmake a copy of the file, using a different name, then remove it.\n\n\ncp hello.txt bye.txt\nls -lth\nrm bye.txt\nls -lth\n\n\n\nmove the file (or rename it). What is the difference between \nmv\n and \ncp\n???\n\n\nmv hello.txt bye.txt\nls -lth\n\n\n\nmake a folder (directory), make a file inside a folder.\n\n\nmkdir testfolder\nls -lth\n#make a folder inside that folder\nmkdir testfolder/inside_test\n#make a file\nnano testfolder/inside_test/inside.txt\nhead testfolder/inside_test/inside.txt\nrm testfolder/inside_test/inside.txt\n\n\n\nthere are a few other commands that you should be familiar with: \nsort\n, \ncat\n, \nclear\n, \ntail\n, \nhistory\n. Try googling and using \nman\n to figure them out.\n\n\nDownloading Data and Stuff\n\n\ndownload something from the web. You're using the \nwget\n command. You're downloading the SwissProt database. See http://www.ebi.ac.uk/uniprot\n\n\nmkdir swissprot\ncd swissprot\nwget ftp://ftp.ebi.ac.uk/pub/databases/uniprot/knowledgebase/uniprot_sprot.fasta.gz\n\n\n\nIt will take a few minutes to download. After it's downloaded, you'll need to extract it. Files ending in \n.gz\n are compressed, just like \n.zip\n, which is a type of file compression you may be more familiar with.\n\n\ngzip -d uniprot_sprot.fasta.gz\n\n\n\n\n\nCan you tell me what type of file this is? Use the commands we used above to look at the 1st few lines.\n\n\n\n\n???\n\n\n\nThere is some info that is complementary to this material found here: \nhttp://swcarpentry.github.io/2014-08-21-upenn/novice/ref/01-shell.html", 
            "title": "Unix"
        }, 
        {
            "location": "/lab_lessons/unix/#lab-1", 
            "text": "During this lab, we will acquaint ourselves with the Unix terminal, learn how to access data, install software, and\u00a0 find things.  it is absolutely critical that you master these skills , so please ask questions if confused.  Step 1: Launch and AMI. For this exercise, a t1.micro will be sufficient.  ssh -i ~/Downloads/your.pem ubuntu@ec2-???-???-???-???.compute-1.amazonaws.com  The machine you are using is Linux Ubuntu: Ubuntu is an operating system you can use (I do) on your laptop or desktop. One of the nice things about this OS is the ability to update the software, easily.\u00a0 The command  sudo apt-get update  checks a server for updates to existing software.  sudo apt-get update  The upgrade command actually installs any of the required updates.  sudo apt-get upgrade  OK, what are these commands?\u00a0  sudo  is the command that tells the computer that we have admin privileges. Try running the commands without the sudo -- it will complain that you don't have admin privileges or something like that.  Careful here, using sudo means that you can do something really bad to your own computer -- like delete everything , so use with caution. It's not a big worry when using AWS, as this is a virtual machine- fixing your worst mistake is as easy as just terminating the instance and restarting.  So now that we have updates the software, lets see how to add new software. Same basic command, but instead of the  update  or  upgrade  command, we're using  install . EASY!!  sudo apt-get -y install tmux git curl gcc make g++ python-dev unzip \\\n    default-jre  After you run this command, try something else - try to install something else. R (a stats package - more on this wonderful software later). The package is named  r-base-core . See if you can install it!! Installing software on Linux is easy (so long as there is a downloadable package - more on when no such package exists later in lab)  BTW, did you notice the  \\  at the end of line 1 in the above code snippett?? That is a special character we use to break up a single line of code over 2 or more lines. You'll see me use this a lot!  OK, lets try our hands at navigating around on the command line - it is not scary!", 
            "title": "Lab 1"
        }, 
        {
            "location": "/lab_lessons/unix/#important-unix-rules", 
            "text": "Everything is case sensitive. Gen711 is not the same as gen711  Spaces in file names should be avoided  The unix $PATH is the collection of locations where the computer looks for executables (programs)   Folders and Files are all you have. If you want to access one of these, you need to tell the computer  EXACTLY  where it is.  /home/macmanes/gen711/exam1_key.txt  will work (assuming you've spelled things correctly, and that the file really exists in that location), but  exam1_key.txt  may not.    Lines that begin with a  #  are comments.", 
            "title": "Important UNIX rules"
        }, 
        {
            "location": "/lab_lessons/unix/#basic-shell-commands", 
            "text": "the  pwd  command returns your current location.  pwd  the  ls  command lists the files and folders present in your current directory.\u00a0 Try  ls -lt  and  ls -lth .  What is the difference between these commands?  Try typing  man ls  to learn about all the different flags.  ls -l  create a file  nano hello.txt\n#The nano text editor will appear -  type something\nThis is my 1st unix file\nCTL-x\ny\n#typing n would get rid of the text you just wrote.  look at the file, there are several ways to look at the file  head -5 hello.txt #this shows you the 1st 5 lines of the file\nmore hello.txt #this shows you the whole file, 1 screen at a time. Space bar to advance, q to quit  make a copy of the file, using a different name, then remove it.  cp hello.txt bye.txt\nls -lth\nrm bye.txt\nls -lth  move the file (or rename it). What is the difference between  mv  and  cp ???  mv hello.txt bye.txt\nls -lth  make a folder (directory), make a file inside a folder.  mkdir testfolder\nls -lth\n#make a folder inside that folder\nmkdir testfolder/inside_test\n#make a file\nnano testfolder/inside_test/inside.txt\nhead testfolder/inside_test/inside.txt\nrm testfolder/inside_test/inside.txt  there are a few other commands that you should be familiar with:  sort ,  cat ,  clear ,  tail ,  history . Try googling and using  man  to figure them out.", 
            "title": "Basic shell commands"
        }, 
        {
            "location": "/lab_lessons/unix/#downloading-data-and-stuff", 
            "text": "download something from the web. You're using the  wget  command. You're downloading the SwissProt database. See http://www.ebi.ac.uk/uniprot  mkdir swissprot\ncd swissprot\nwget ftp://ftp.ebi.ac.uk/pub/databases/uniprot/knowledgebase/uniprot_sprot.fasta.gz  It will take a few minutes to download. After it's downloaded, you'll need to extract it. Files ending in  .gz  are compressed, just like  .zip , which is a type of file compression you may be more familiar with.  gzip -d uniprot_sprot.fasta.gz   Can you tell me what type of file this is? Use the commands we used above to look at the 1st few lines.   ???  There is some info that is complementary to this material found here:  http://swcarpentry.github.io/2014-08-21-upenn/novice/ref/01-shell.html", 
            "title": "Downloading Data and Stuff"
        }
    ]
}